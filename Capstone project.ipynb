{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Natural language processing has been a topic of conversation since Alan Turing first designed a test that would involve a judge conversing with both a human and a machine and telling which one is the machine and which is the human. A machine is said to “pass” the test if the judge cannot tell the two apart. For years, the task of building a machine that passes this test has been – and continues to be - a very difficult task for computers and their programmers to accomplish. The main difficulty is due to all of the various rules and vocabularies we apply almost innately in our everyday conversations. For example, some words mean different things in different contexts.\n",
    "    \n",
    "\tWe can evaluate how well our models perform using questions that have already been answered by humans, by comparing these answers to the answers our machine gives. We can then compare the words and sentence structure of the machine’s answer to that of the human’s. For example, we could give the machine a score of 1 every time it uses a word in its answer as the human does, perhaps with the caveat that it only gets the point for words that succeed another matching word (or are the first word in the sentence.)\n",
    "    \n",
    "\tHowever, there are many ways to build a program that can try to understand and speak human language. One of the earliest applications of machine learning for this problem involved question classification. Researchers realized as far back as 1992 that in order to answer a question, a machine needed to be able to understand the constraints placed on a question. To do this, Drs. Xin Li and Dan Roth proposed a hierarchical classifier that would classify a question. (https://dl.acm.org/citation.cfm?id=1072378).\n",
    "    \n",
    "\tThe problem, of course, with this approach is that each possible question needs to have a classification. Furthermore, since our machine learning classifier needs to understand how the type of question affects the answer, it needs to be trained to understand the types of words (e.g noun, adjective), the sentence structures, etc. For example, for the question, “How does the human heart work?” it needs to first understand how to answer a question on how anything works. Only then can it answer the question of how the human heart works. \n",
    "    \n",
    "    What I propose is that instead of concerning ourselves with how a question is classified or the types of words in the question, or its sentence structure, we can instead train a machine to answer a question by classifying the question in terms of how it would be answered. This allows us to train our program on questions for which we may not have defined a classification. For example, to answer a question from our SQuAD dataset, the program wouldn’t necessarily learn that a question is related to an article, but that it is related to another question and to the answer we’re looking for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task in any machine learning setup is to define our inputs and outputs.\n",
    "For this task, it is readily obvious that our questions should be inputs and our answers should be the outputs.\n",
    "\n",
    "However, in order for our bot to be able to formulate an answer,\n",
    "any neural network we create to generate answers needs information from which it can pick out an answer.\n",
    "\n",
    "Therefore, we will use two neural networks. The first neural network's job will be to identify which article in the dataset best answers a given question. The second neural network's job will be to take as input both the question posed and the relevant article, and then output the portion of the article that answers the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing step, we will create two dictionaries. One will be used to map questions to articles, while the other will be used to map questions and the contents of the articles to the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading JSON dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading data.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "questions_articleTitles = {} # Stores questions and article titles.\n",
    "questions_answers = {} # Stores questions with their associated answers, stored as integers representing where an ansser begins and ends\n",
    "with open('train-v1.1.json') as file:\n",
    "    data = json.load(file)\n",
    "    articles = data[\"data\"]\n",
    "    # Iterate through articles, looking for question/answer pairs.\n",
    "    for article in articles:\n",
    "        article_title = article[\"title\"]\n",
    "        article_paragraphs = article[\"paragraphs\"]\n",
    "        for paragraph in article_paragraphs:\n",
    "            qas_pairs = paragraph[\"qas\"]\n",
    "            for qas_pair in qas_pairs:\n",
    "                # Note: There's another attribute called \"context\", which may come in handy.\n",
    "                answer = qas_pair[\"answers\"][0]\n",
    "                answer_text = answer[\"text\"]\n",
    "                # Get where to find the answers.\n",
    "                answer_start = answer[\"answer_start\"]\n",
    "                answer_end = answer_start + len(answer_text) - 1\n",
    "                question = qas_pair[\"question\"]\n",
    "                questions_answers[question] = answer_text\n",
    "                questions_articleTitles[question] = article_title\n",
    "print(\"Finished loading data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87355, 'question-article pairs.')\n",
      "(87355, 'question-answer pairs.')\n"
     ]
    }
   ],
   "source": [
    "print(len(questions_articleTitles), \"question-article pairs.\")\n",
    "print(len(questions_answers), \"question-answer pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing using scikit-learn and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X prepared\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Combine all text values to fit into the vectorizer.\n",
    "questions = questions_articleTitles.keys()\n",
    "articleTitles = questions_articleTitles.values()\n",
    "answers = questions_answers.values()\n",
    "X = questions\n",
    "m = map(lambda x: x.lower(),X) # Make sure each element in combined is a string.\n",
    "# Note: It could be useful to add article titles to the neural network's input \n",
    "#    to ensure the neural network doesn't suggest an article that doesn't answer the question, or doesn't exist.\n",
    "print(\"X prepared\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section turns inputs into n-grams. Note that order is not necessarily preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like', 'like cheese']\n",
      "[u'from Johns', u'Did Eran', u'or against', u'Public Health', u'Johns Hopkins', u'Health argue', u'Elhaik, from', u'Eran Elhaik,', u'Hopkins University', u'of Public', u'University School', u'against Khazar', u'School of', u'for or', u'Khazar descent?', u'argue for']\n",
      "Done computing n-grams and n-gram vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# Get skipgrams.\n",
    "def ngrams(input):\n",
    "    _ngrams = []\n",
    "    sentenceWords = input.split(\" \")\n",
    "    for i in range(0,len(sentenceWords) - 1):\n",
    "        ngram = sentenceWords[i] + \" \"\n",
    "        ngram = ngram + sentenceWords[i + 1]\n",
    "        _ngrams.append(ngram)\n",
    "    return list(set(_ngrams))\n",
    "print(ngrams(\"I like cheese\"))\n",
    "X_ngrams = map(ngrams,X)\n",
    "print(X_ngrams[0])\n",
    "articleTitles_ngrams = map(ngrams,articleTitles)\n",
    "answers_ngrams = map(ngrams,answers)\n",
    "combined_ngrams = X_ngrams + articleTitles_ngrams + answers_ngrams\n",
    "print(\"Done computing n-grams and n-gram vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed n-gram vocabulary; sample: or against\n"
     ]
    }
   ],
   "source": [
    "# Compute n-gram vocabulary.\n",
    "combinedString = [string for element in combined_ngrams for string in element]\n",
    "print(\"Computed n-gram vocabulary; sample: \" + combinedString[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Duplicate term in vocabulary: u'at the'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-61fb705bd34b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_first_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombinedString\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx_first_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX_first_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_first_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0marticleTitles_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticleTitles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    831\u001b[0m                 \"string object received.\")\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m         \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0mmin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_validate_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Duplicate term in vocabulary: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Duplicate term in vocabulary: u'at the'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vectorize all strings.\n",
    "# TODO: Fit vectorizer on ngram vocabulary.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "x_first_vectorizer = CountVectorizer(vocabulary=combinedString)\n",
    "x_first_vectorizer.fit(X)\n",
    "X_first_vectorized = x_first_vectorizer.transform(X)\n",
    "articleTitles_vectorized = vectorizer.transform(articleTitles)\n",
    "print(\"Vectorized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting training data into training + cross-validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_first_array normalized.\n",
      "First neural network X shape: (87355, 36752)\n",
      "First neural network input shape: (36752,)\n",
      "Article titles array shape: (87355, 36752)\n"
     ]
    }
   ],
   "source": [
    "# convert data to numpy arrays, get input shape\n",
    "X_first_array = X_first_vectorized.toarray()\n",
    "from sklearn.preprocessing import normalize\n",
    "X_first_array = normalize(X_first_array)\n",
    "print(\"X_first_array normalized.\")\n",
    "articleTitles_array = articleTitles_vectorized.toarray()\n",
    "inputShape_first = X_first_array.shape[1:] # Input shape of the first neural network.\n",
    "print(\"First neural network X shape: \" + str(X_first_array.shape))\n",
    "print(\"First neural network input shape: \" + str(inputShape_first))\n",
    "print(\"Article titles array shape: \" + str(articleTitles_array.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_107 (Dense)            (None, 256)               9408768   \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 256)               4352      \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 36752)             9445264   \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 36752)             0         \n",
      "=================================================================\n",
      "Total params: 18,877,440\n",
      "Trainable params: 18,877,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Activation,InputLayer,Input,BatchNormalization\n",
    "\n",
    "questions_article_model = Sequential()\n",
    "questions_article_model.add(Dense(256,input_shape=inputShape_first))\n",
    "questions_article_model.add(Dense(64))\n",
    "questions_article_model.add(Dense(32))\n",
    "questions_article_model.add(Dense(16))\n",
    "questions_article_model.add(Dense(256))\n",
    "questions_article_model.add(Dense(articleTitles_array.shape[1]))\n",
    "questions_article_model.add(Activation(\"sigmoid\"))\n",
    "questions_article_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_article_model.compile(\"SGD\",\"categorical_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65516 samples, validate on 21839 samples\n",
      "Epoch 1/8\n",
      "23552/65516 [=========>....................] - ETA: 1:39 - loss: 5.1707 - acc: 0.0084"
     ]
    }
   ],
   "source": [
    "questions_article_model.fit(X_first_array,articleTitles_array,epochs=8,validation_split=0.25,callbacks=[checkpoint],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
