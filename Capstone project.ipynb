{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Natural language processing has been a topic of conversation since Alan Turing first designed a test that would involve a judge conversing with both a human and a machine and telling which one is the machine and which is the human. A machine is said to “pass” the test if the judge cannot tell the two apart. For years, the task of building a machine that passes this test has been – and continues to be - a very difficult task for computers and their programmers to accomplish. The main difficulty is due to all of the various rules and vocabularies we apply almost innately in our everyday conversations. For example, some words mean different things in different contexts.\n",
    "    \n",
    "\tWe can evaluate how well our models perform using questions that have already been answered by humans, by comparing these answers to the answers our machine gives. We can then compare the words and sentence structure of the machine’s answer to that of the human’s. For example, we could give the machine a score of 1 every time it uses a word in its answer as the human does, perhaps with the caveat that it only gets the point for words that succeed another matching word (or are the first word in the sentence.)\n",
    "    \n",
    "\tHowever, there are many ways to build a program that can try to understand and speak human language. One of the earliest applications of machine learning for this problem involved question classification. Researchers realized as far back as 1992 that in order to answer a question, a machine needed to be able to understand the constraints placed on a question. To do this, Drs. Xin Li and Dan Roth proposed a hierarchical classifier that would classify a question. (https://dl.acm.org/citation.cfm?id=1072378).\n",
    "    \n",
    "\tThe problem, of course, with this approach is that each possible question needs to have a classification. Furthermore, since our machine learning classifier needs to understand how the type of question affects the answer, it needs to be trained to understand the types of words (e.g noun, adjective), the sentence structures, etc. For example, for the question, “How does the human heart work?” it needs to first understand how to answer a question on how anything works. Only then can it answer the question of how the human heart works. \n",
    "    \n",
    "    What I propose is that instead of concerning ourselves with how a question is classified or the types of words in the question, or its sentence structure, we can instead train a machine to answer a question by classifying the question in terms of how it would be answered. This allows us to train our program on questions for which we may not have defined a classification. For example, to answer a question from our SQuAD dataset, the program wouldn’t necessarily learn that a question is related to an article, but that it is related to another question and to the answer we’re looking for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task in any machine learning setup is to define our inputs and outputs.\n",
    "For this task, it is readily obvious that our questions should be inputs and our answers should be the outputs.\n",
    "\n",
    "However, in order for our bot to be able to formulate an answer,\n",
    "any neural network we create to generate answers needs information from which it can pick out an answer.\n",
    "\n",
    "Therefore, we will use two neural networks. The first neural network's job will be to identify which article in the dataset best answers a given question. The second neural network's job will be to take as input both the question posed and the relevant article, and then output the portion of the article that answers the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Just to verify that everything's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Activation,InputLayer,Input,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Verify we are using GPU.\n",
    "from tensorflow.python.client import device_lib\n",
    "import sys\n",
    "assert sys.version_info[0] >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing step, we will create two dictionaries. One will be used to map questions to articles, while the other will be used to map questions and the contents of the articles to the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading JSON dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading data.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "questions_articleTitles = {} # Stores questions and article titles.\n",
    "questions_answers = {} # Stores questions with their associated answers, stored as integers representing where an ansser begins and ends\n",
    "# TODO: Convert unicode to string.\n",
    "with open('train-v1.1.json') as file:\n",
    "    data = json.load(file)\n",
    "    articles = data[\"data\"]\n",
    "    # Iterate through articles, looking for question/answer pairs.\n",
    "    for article in articles:\n",
    "        article_title = article[\"title\"].encode('ascii','replace')# Converts Unicode object to string.\n",
    "        article_paragraphs = article[\"paragraphs\"]\n",
    "        for paragraph in article_paragraphs:\n",
    "            qas_pairs = paragraph[\"qas\"]\n",
    "            for qas_pair in qas_pairs:\n",
    "                # Note: There's another attribute called \"context\", which may come in handy.\n",
    "                answer = qas_pair[\"answers\"][0]\n",
    "                answer_text = answer[\"text\"].encode('ascii','replace') # Converts Unicode object to string.\n",
    "                # Get where to find the answers.\n",
    "                #answer_start = answer[\"answer_start\"]\n",
    "                #answer_end = answer_start + len(answer_text) - 1\n",
    "                question = qas_pair[\"question\"].encode('ascii','replace')\n",
    "                questions_answers[question] = answer_text\n",
    "                questions_articleTitles[question] = article_title\n",
    "\n",
    "print(\"Finished loading data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87351 question-article pairs.\n",
      "87351 question-answer pairs.\n"
     ]
    }
   ],
   "source": [
    "print(len(questions_articleTitles), \"question-article pairs.\")\n",
    "print(len(questions_answers), \"question-answer pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing using scikit-learn and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X prepared\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine all text values to fit into the vectorizer.\n",
    "questions = questions_articleTitles.keys()\n",
    "articleTitles = questions_articleTitles.values()\n",
    "answers = questions_answers.values()\n",
    "X = questions\n",
    "m = map(lambda x: x.lower(),X) # Make sure each element in combined is a string.\n",
    "# Note: It could be useful to add article titles to the neural network's input \n",
    "#    to ensure the neural network doesn't suggest an article that doesn't answer the question, or doesn't exist.\n",
    "print(\"X prepared\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section turns inputs into n-grams. Note that order is not necessarily preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized. Sample from vocabulary: uthmanic\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vectorize all strings.\n",
    "# TODO: Fit vectorizer on ngram vocabulary.\n",
    "first_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "first_vectorizer.fit(X)\n",
    "X_first_vectorized = first_vectorizer.transform(X)\n",
    "\n",
    "articleTitles_vectorized = first_vectorizer.transform(articleTitles)\n",
    "print(\"Vectorized. Sample from vocabulary: \" + str(list(first_vectorizer.vocabulary_.keys())[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting training data into training + cross-validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First neural network X shape: (87351, 333113)\n",
      "First neural network input shape: (333113,)\n",
      "Article titles array shape: (87351, 333113)\n"
     ]
    }
   ],
   "source": [
    "# convert data to numpy arrays, get input shape\n",
    "X_first_array = X_first_vectorized.toarray()\n",
    "articleTitles_array = articleTitles_vectorized.toarray()\n",
    "inputShape_first = X_first_array.shape[1:] # Input shape of the first neural network.\n",
    "print(\"First neural network X shape: \" + str(X_first_array.shape))\n",
    "print(\"First neural network input shape: \" + str(inputShape_first))\n",
    "print(\"Article titles array shape: \" + str(articleTitles_array.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 8)                 2664912   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 333113)            8327825   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 333113)            0         \n",
      "=================================================================\n",
      "Total params: 10,993,321\n",
      "Trainable params: 10,993,305\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "questions_article_model = Sequential()\n",
    "questions_article_model.add(Dense(8,input_shape=inputShape_first))\n",
    "questions_article_model.add(BatchNormalization())\n",
    "questions_article_model.add(Dense(16))\n",
    "questions_article_model.add(Dense(24))\n",
    "questions_article_model.add(Dense(articleTitles_array.shape[1]))\n",
    "questions_article_model.add(Activation(\"sigmoid\"))\n",
    "questions_article_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_article_model.compile(\"adam\",\"categorical_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65513 samples, validate on 21838 samples\n",
      "Epoch 1/8\n",
      "59680/65513 [==========================>...] - ETA: 1:15 - loss: 2.9918 - acc: 0.0196"
     ]
    }
   ],
   "source": [
    "questions_article_model.fit(X_first_array,articleTitles_array,epochs=8,validation_split=0.25,callbacks=[checkpoint],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
