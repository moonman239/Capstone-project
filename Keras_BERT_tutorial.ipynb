{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_BERT_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moonman239/Capstone-project/blob/master/Keras_BERT_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdU_O3BUzfbK",
        "colab_type": "text"
      },
      "source": [
        "#Loading Google BERT models into Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZVc9dvZzvSG",
        "colab_type": "text"
      },
      "source": [
        "You may have heard of Google's BERT. This is among the best models, topping the charts for the SQuAD 2.0 dataset and achieving a >80% score on the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_sBMQbC0BKk",
        "colab_type": "text"
      },
      "source": [
        "Since this is a tutorial, we won't be getting much into the nitty-gritty of how BERT works. Suffice it to say, for now, that BERT takes a unique approach. Suppose we're training BERT on the string \"I like dogs.\" BERT will train on:\n",
        "1) \"I like dogs\"\n",
        "2) \"dogs like I\"\n",
        "Additionally, when training BERT, Google trains by replacing various words in the sentences with \"masks,\" e.g.: \"[mask] like dogs.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD1mGuteE1SZ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdOQ1afZ1HbW",
        "colab_type": "text"
      },
      "source": [
        "Enough talk, let's code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P9rEGb712UZ",
        "colab_type": "text"
      },
      "source": [
        "We'll go ahead and import our modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfaFd8Aj181r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "cebf9251-d4eb-4a31-dc54-be4c5d981abe"
      },
      "source": [
        "!pip install -U bert-serving-server bert-serving-client # BERT server/client\n",
        "!pip install wget # Used for downloading the BERT model.\n",
        "import wget\n",
        "import os\n",
        "import urllib\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import zipfile"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-serving-server\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/3e/44d79e1a739b8619760051410c61af67f95477c87fbe43e3e9426427feb5/bert_serving_server-1.9.1-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.9MB/s \n",
            "\u001b[?25hCollecting bert-serving-client\n",
            "  Downloading https://files.pythonhosted.org/packages/77/24/d17de2bfe84db45be0080f01f3819a821db4bfbd9b927d66c828277ebd02/bert_serving_client-1.9.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.16.4)\n",
            "Collecting pyzmq>=17.1.0 (from bert-serving-server)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/04/f6f0fa20b698b29c6e6b1d6b4b575c12607b0abf61810aab1df4099988c6/pyzmq-18.0.1-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.1.0)\n",
            "Collecting GPUtil>=1.3.0 (from bert-serving-server)\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: pyzmq, GPUtil, bert-serving-server, bert-serving-client\n",
            "  Found existing installation: pyzmq 17.0.0\n",
            "    Uninstalling pyzmq-17.0.0:\n",
            "      Successfully uninstalled pyzmq-17.0.0\n",
            "Successfully installed GPUtil-1.4.0 bert-serving-client-1.9.1 bert-serving-server-1.9.1 pyzmq-18.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "zmq"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-pWbdFn1WNX",
        "colab_type": "text"
      },
      "source": [
        "Next, let's set everything up. Let's begin by defining what model we'd like to work with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhgLPqit1jkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_model_name = \"wwm_uncased_L-24_H-1024_A-16\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW_uc7u62D-a",
        "colab_type": "text"
      },
      "source": [
        "Okay, now let's go ahead and download that model from GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP1OpHNc2LEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5b849b0-af1c-40d0-8bb4-a2c9eb60d63a"
      },
      "source": [
        "bert_model_file_name = bert_model_name + \".zip\"\n",
        "online_bert_path = \"https://storage.googleapis.com/bert_models/2019_05_30/\" + bert_model_file_name\n",
        "if not os.path.isfile(bert_model_file_name):\n",
        "  wget.download(online_bert_path)\n",
        "  zipfile.ZipFile(bert_model_file_name,'r').extractall()\n",
        "print(os.path.abspath(bert_model_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/wwm_uncased_L-24_H-1024_A-16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8tk1LK64pcN",
        "colab_type": "text"
      },
      "source": [
        "If there are no errors here, great! We are ready to work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7OYtuXN42eD",
        "colab_type": "text"
      },
      "source": [
        "## Load pretrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H4o7hQCBpWh",
        "colab_type": "text"
      },
      "source": [
        "Let's start the BERT server."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnVGJRNg4xMj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1356
        },
        "outputId": "e2a91fa7-02d9-4801-941c-c0135f4fab27"
      },
      "source": [
        "!bert-serving-start -model_dir /content/wwm_uncased_L-24_H-1024_A-16 -num_worker=4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: /usr/local/bin/bert-serving-start -model_dir /content/wwm_uncased_L-24_H-1024_A-16 -num_worker=4\n",
            "                 ARG   VALUE\n",
            "__________________________________________________\n",
            "           ckpt_name = bert_model.ckpt\n",
            "         config_name = bert_config.json\n",
            "                cors = *\n",
            "                 cpu = False\n",
            "          device_map = []\n",
            "       do_lower_case = True\n",
            "  fixed_embed_length = False\n",
            "                fp16 = False\n",
            " gpu_memory_fraction = 0.5\n",
            "       graph_tmp_dir = None\n",
            "    http_max_connect = 10\n",
            "           http_port = None\n",
            "        mask_cls_sep = False\n",
            "      max_batch_size = 256\n",
            "         max_seq_len = 25\n",
            "           model_dir = /content/wwm_uncased_L-24_H-1024_A-16\n",
            "          num_worker = 4\n",
            "       pooling_layer = [-2]\n",
            "    pooling_strategy = REDUCE_MEAN\n",
            "                port = 5555\n",
            "            port_out = 5556\n",
            "       prefetch_size = 10\n",
            " priority_batch_size = 16\n",
            "show_tokens_to_client = False\n",
            "     tuned_model_dir = None\n",
            "             verbose = False\n",
            "                 xla = False\n",
            "\n",
            "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 66]:freeze, optimize and export graph, could take a while...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 52]:model config: /content/wwm_uncased_L-24_H-1024_A-16/bert_config.json\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 55]:checkpoint: /content/wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 59]:build graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:128]:load parameters from checkpoint...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:optimize...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:140]:freeze...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:145]:write graph to a tmp file: /tmp/tmpnrx2grm7\n",
            "tcmalloc: large alloc 1286365184 bytes == 0xb7a8000 @  0x7f9c8a8f61e7 0x59213c 0x7f9c74039650 0x5030d5 0x506859 0x504c28 0x58650d 0x59ebbe 0x507c17 0x504c28 0x58650d 0x59ebbe 0x507c17 0x502209 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x501945 0x591461 0x54b813 0x555421 0x5a730c 0x503073 0x506859 0x502209 0x502f3d\n",
            "tcmalloc: large alloc 1286365184 bytes == 0x5826e000 @  0x7f9c8a8f8887 0x7f9c891eebf9 0x7f9c891eefc2 0x7f9c891ef62e 0x7f9c7734eb0c 0x7f9c77378dc5 0x502d6f 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x58650d 0x59ebbe 0x507c17 0x504c28 0x58650d 0x59ebbe 0x507c17 0x502209 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x501945 0x591461 0x54b813\n",
            "I:\u001b[35mVENTILATOR\u001b[0m:[__i:__i: 74]:optimized graph is stored at: /tmp/tmpnrx2grm7\n",
            "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:128]:bind all sockets\n",
            "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:132]:open 8 ventilator-worker sockets\n",
            "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ru:135]:start the sink\n",
            "I:\u001b[35mVENTILATOR\u001b[0m:[__i:_ge:221]:get devices\n",
            "I:\u001b[32mSINK\u001b[0m:[__i:_ru:305]:ready\n",
            "Exception in thread Thread-1:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/bert_serving/server/__init__.py\", line 114, in run\n",
            "    self._run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/decorators.py\", line 75, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/decorators.py\", line 75, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/decorators.py\", line 75, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/bert_serving/server/zmq_decor.py\", line 27, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/bert_serving/server/__init__.py\", line 142, in _run\n",
            "    device_map = self._get_device_map()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/bert_serving/server/__init__.py\", line 227, in _get_device_map\n",
            "    num_all_gpu = len(GPUtil.getGPUs())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/GPUtil/GPUtil.py\", line 102, in getGPUs\n",
            "    deviceIds = int(vals[i])\n",
            "ValueError: invalid literal for int() with base 10: \"NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}