{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone_project_merged.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ecPo0Jetse39",
        "oq_sLBsSse3_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoIQFdjR-Lid",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDXUu5iW-Lij",
        "colab_type": "text"
      },
      "source": [
        "Load Python modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGcmXwG--Lio",
        "colab_type": "code",
        "outputId": "7ddbeca7-b87e-4297-8bbc-68026bf8a30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.python.client import device_lib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding,Dropout,Lambda,Dense,Input,LSTM,Concatenate,Flatten,Add,Reshape,GlobalAveragePooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import sys\n",
        "assert sys.version_info[0] >= 3\n",
        "!pip install keras_bert\n",
        "print(\"Set up!\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_bert\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/f1/02896ce76c132761ba21fed5ea9461ffa7c69e7a9beb76ff7657d69f98f3/keras-bert-0.55.1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras_bert) (1.16.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_bert) (2.2.4)\n",
            "Collecting keras-pos-embd==0.10.0 (from keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/01/e4/8b8519779d84c412c2c1bbf4637066800c3b04463da059a5d220fa904133/keras-pos-embd-0.10.0.tar.gz\n",
            "Collecting keras-transformer==0.23.0 (from keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/31/0d/0b62504dc9a6377d035b651b9222c95f9094c002bdcb17162ad4863de6ba/keras-transformer-0.23.0.tar.gz\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (1.0.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (1.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (1.0.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (1.12.0)\n",
            "Collecting keras-multi-head==0.20.0 (from keras-transformer==0.23.0->keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/0f/f1a66974db9c328ba675c1df63f8a68c5c0f3e181f1e74db4f3b1a72a6df/keras-multi-head-0.20.0.tar.gz\n",
            "Collecting keras-layer-normalization==0.12.0 (from keras-transformer==0.23.0->keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/76/42878fe46bff8458d8aa1da50bfdf705d632d33dbb7b60db52a06faf2dad/keras-layer-normalization-0.12.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward==0.5.0 (from keras-transformer==0.23.0->keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/51/a8/b9dec35117d0fb16d07bd8c216540c4ab42e93dd3d9ca43345cb01bbdc3f/keras-position-wise-feed-forward-0.5.0.tar.gz\n",
            "Collecting keras-embed-sim==0.4.0 (from keras-transformer==0.23.0->keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/80/c7573cdb2344bc93824c9327f390e40dabe32dc895087651090baa134999/keras-embed-sim-0.4.0.tar.gz\n",
            "Collecting keras-self-attention==0.41.0 (from keras-multi-head==0.20.0->keras-transformer==0.23.0->keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-pos-embd, keras-transformer, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/2e/bf/7b92973a0164059cee3deeb718295911baef8bb62dcc344ca2\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/bc/07/35f14835eaaca28a99d33b1c6f36df6aeae052161abb8af465\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/89/ee/620c1c8de642ab21ac8236b0dc77cab7a0dce2f59fc88cd468\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/49/02/4eda210bc4c37ff1d45311665bceb790881dbea92b27b025a5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/9b/9e/f4072915f660e90bb3638332276f4de80476f3afcb5d010d6f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/58/cc/007f963a01c8304c793f9e887d50ff6e7c495343287c01a6a2\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/71/f6/2c51739d83b0c874a491698ed345293dfa62c7ce52798b86bd\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-bert keras-pos-embd keras-transformer keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.55.1 keras-embed-sim-0.4.0 keras-layer-normalization-0.12.0 keras-multi-head-0.20.0 keras-pos-embd-0.10.0 keras-position-wise-feed-forward-0.5.0 keras-self-attention-0.41.0 keras-transformer-0.23.0\n",
            "Set up!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXFuFons-LjA",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqjrxOes-LjD",
        "colab_type": "text"
      },
      "source": [
        "For the preprocessing step, we will create two dictionaries. One will be used to map questions to articles, while the other will be used to map questions and the contents of the articles to the answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8UbZDe--LjG",
        "colab_type": "text"
      },
      "source": [
        "### Loading JSON datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I79-hb5dCBfy",
        "colab_type": "code",
        "outputId": "9f3a21cb-0818-4c43-b371-1f6496110670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "currentDirectory = os.getcwd()\n",
        "from google.colab import drive\n",
        "driveBase = os.path.join(os.path.dirname(os.path.abspath(\"train-v1.1.json\")),\"drive\")\n",
        "drive.mount(driveBase)\n",
        "hardDrive = os.path.join(driveBase,\"My Drive\")\n",
        "os.chdir(hardDrive)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fgU382EF26d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import re\n",
        "regex = re.compile(r'\\W+')\n",
        "def readFile(filename):\n",
        "  with open(filename) as file:\n",
        "    fields = []\n",
        "    JSON = json.loads(file.read())\n",
        "    for article in JSON[\"data\"]:\n",
        "      articleTitle = article[\"title\"]\n",
        "      for paragraph in article[\"paragraphs\"]:\n",
        "        paragraphContext = paragraph[\"context\"]\n",
        "        for qas in paragraph[\"qas\"]:\n",
        "          question = qas[\"question\"]\n",
        "          for answer in qas[\"answers\"]:\n",
        "            fields.append({\"question\":question,\"answer_text\":answer[\"text\"],\"answer_start\":answer[\"answer_start\"],\"paragraph_context\":paragraphContext,\"article_title\":articleTitle})\n",
        "  fields = pd.DataFrame(fields)\n",
        "  fields[\"question\"] = fields[\"question\"].str.replace(regex,\" \")\n",
        "  assert not (fields[\"question\"].str.contains(\"catalanswhat\").any())\n",
        "  fields[\"paragraph_context\"] = fields[\"paragraph_context\"].str.replace(regex,\" \")\n",
        "  fields[\"answer_text\"] = fields[\"answer_text\"].str.replace(regex,\" \")\n",
        "  assert not (fields[\"paragraph_context\"].str.contains(\"catalanswhat\").any())\n",
        "  fields[\"article_title\"] = fields[\"article_title\"].str.replace(\"_\",\" \")\n",
        "  assert not (fields[\"article_title\"].str.contains(\"catalanswhat\").any())\n",
        "  return fields"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Ql0r8fOpCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainingData = readFile(\"train-v1.1.json\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XufEvRe0ap_R",
        "colab_type": "code",
        "outputId": "fb75d0c8-98ba-4e6f-f738-c221fbe40ce5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2989
        }
      },
      "source": [
        "trainingData"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>article_title</th>\n",
              "      <th>paragraph_context</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>515</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>Architecturally the school has a Catholic char...</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>188</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>Architecturally the school has a Catholic char...</td>\n",
              "      <td>What is in front of the Notre Dame Main Building</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>279</td>\n",
              "      <td>the Main Building</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>Architecturally the school has a Catholic char...</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>381</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>Architecturally the school has a Catholic char...</td>\n",
              "      <td>What is the Grotto at Notre Dame</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>92</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>Architecturally the school has a Catholic char...</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>248</td>\n",
              "      <td>September 1876</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>As at most other universities Notre Dame s stu...</td>\n",
              "      <td>When did the Scholastic Magazine of Notre dame...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>441</td>\n",
              "      <td>twice</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>As at most other universities Notre Dame s stu...</td>\n",
              "      <td>How often is Notre Dame s the Juggler published</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>598</td>\n",
              "      <td>The Observer</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>As at most other universities Notre Dame s stu...</td>\n",
              "      <td>What is the daily student paper at Notre Dame ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>126</td>\n",
              "      <td>three</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>As at most other universities Notre Dame s stu...</td>\n",
              "      <td>How many student news papers are found at Notr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>908</td>\n",
              "      <td>1987</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>As at most other universities Notre Dame s stu...</td>\n",
              "      <td>In what year did the student paper Common Sens...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>119</td>\n",
              "      <td>Rome</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The university is the major seat of the Congre...</td>\n",
              "      <td>Where is the headquarters of the Congregation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>145</td>\n",
              "      <td>Moreau Seminary</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The university is the major seat of the Congre...</td>\n",
              "      <td>What is the primary seminary of the Congregati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>234</td>\n",
              "      <td>Old College</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The university is the major seat of the Congre...</td>\n",
              "      <td>What is the oldest structure at Notre Dame</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>356</td>\n",
              "      <td>Retired priests and brothers</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The university is the major seat of the Congre...</td>\n",
              "      <td>What individuals live at Fatima House at Notre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>675</td>\n",
              "      <td>Buechner Prize for Preaching</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The university is the major seat of the Congre...</td>\n",
              "      <td>Which prize did Frederick Buechner create</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>487</td>\n",
              "      <td>eight</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The College of Engineering was established in ...</td>\n",
              "      <td>How many BS level degrees are offered in the C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>46</td>\n",
              "      <td>1920</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The College of Engineering was established in ...</td>\n",
              "      <td>In what year was the College of Engineering at...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>126</td>\n",
              "      <td>the College of Science</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The College of Engineering was established in ...</td>\n",
              "      <td>Before the creation of the College of Engineer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>271</td>\n",
              "      <td>five</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The College of Engineering was established in ...</td>\n",
              "      <td>How many departments are within the Stinson Re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>155</td>\n",
              "      <td>the 1870s</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The College of Engineering was established in ...</td>\n",
              "      <td>The College of Science began to offer civil en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>496</td>\n",
              "      <td>Learning Resource Center</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>All of Notre Dame s undergraduate students are...</td>\n",
              "      <td>What entity provides help with the management ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>68</td>\n",
              "      <td>five</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>All of Notre Dame s undergraduate students are...</td>\n",
              "      <td>How many colleges for undergraduates are at No...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>155</td>\n",
              "      <td>The First Year of Studies program</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>All of Notre Dame s undergraduate students are...</td>\n",
              "      <td>What was created at Notre Dame in 1962 to assi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>647</td>\n",
              "      <td>U S News World Report</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>All of Notre Dame s undergraduate students are...</td>\n",
              "      <td>Which organization declared the First Year of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>358</td>\n",
              "      <td>1924</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The university first offered graduate degrees ...</td>\n",
              "      <td>The granting of Doctorate degrees first occurr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>624</td>\n",
              "      <td>Master of Divinity</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The university first offered graduate degrees ...</td>\n",
              "      <td>What type of degree is an M Div</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1163</td>\n",
              "      <td>Alliance for Catholic Education</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The university first offered graduate degrees ...</td>\n",
              "      <td>Which program at Notre Dame offers a Master of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>92</td>\n",
              "      <td>1854</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The university first offered graduate degrees ...</td>\n",
              "      <td>In what year was a Master of Arts course first...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>757</td>\n",
              "      <td>Department of Pre Professional Studies</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The university first offered graduate degrees ...</td>\n",
              "      <td>Which department at Notre Dame is the only one...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4</td>\n",
              "      <td>Joan B Kroc Institute for International Peace ...</td>\n",
              "      <td>University of Notre Dame</td>\n",
              "      <td>The Joan B Kroc Institute for International Pe...</td>\n",
              "      <td>What institute at Notre Dame studies the reaso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87569</th>\n",
              "      <td>209</td>\n",
              "      <td>Gyaneshwar</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Sikhism is practiced primarily in Gurudwara at...</td>\n",
              "      <td>Where can a temple of the Jain faith be found</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87570</th>\n",
              "      <td>355</td>\n",
              "      <td>300</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Sikhism is practiced primarily in Gurudwara at...</td>\n",
              "      <td>Kathmandu valley is home to about how many Bah...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87571</th>\n",
              "      <td>427</td>\n",
              "      <td>Shantinagar Baneshwor</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Sikhism is practiced primarily in Gurudwara at...</td>\n",
              "      <td>Where is the Baha i national office located in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87572</th>\n",
              "      <td>633</td>\n",
              "      <td>4 2</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Sikhism is practiced primarily in Gurudwara at...</td>\n",
              "      <td>About what percentage of the Nepali population...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87573</th>\n",
              "      <td>728</td>\n",
              "      <td>170</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Sikhism is practiced primarily in Gurudwara at...</td>\n",
              "      <td>About how many Christian houses of worship exi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87574</th>\n",
              "      <td>46</td>\n",
              "      <td>Tribhuwan</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Institute of Medicine the central college of T...</td>\n",
              "      <td>Of what university is the Institute of Medicin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87575</th>\n",
              "      <td>123</td>\n",
              "      <td>Maharajgunj</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Institute of Medicine the central college of T...</td>\n",
              "      <td>In what part of Kathmandu is the Institute of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87576</th>\n",
              "      <td>219</td>\n",
              "      <td>1978</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Institute of Medicine the central college of T...</td>\n",
              "      <td>When did the Institute of Medicine begin to of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87577</th>\n",
              "      <td>425</td>\n",
              "      <td>Kathmandu University School of Medical Sciences</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Institute of Medicine the central college of T...</td>\n",
              "      <td>What does KUSMS stand for</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87578</th>\n",
              "      <td>377</td>\n",
              "      <td>National Academy of Medical Sciences</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Institute of Medicine the central college of T...</td>\n",
              "      <td>What institution of tertiary education is know...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87579</th>\n",
              "      <td>0</td>\n",
              "      <td>Football</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Football and Cricket are the most popular spor...</td>\n",
              "      <td>Along with cricket what sport is highly popula...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87580</th>\n",
              "      <td>160</td>\n",
              "      <td>All Nepal Football Association</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Football and Cricket are the most popular spor...</td>\n",
              "      <td>What body oversees soccer in Nepal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87581</th>\n",
              "      <td>498</td>\n",
              "      <td>25 000</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Football and Cricket are the most popular spor...</td>\n",
              "      <td>How many people can fit in Dasarath Rangasala ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87582</th>\n",
              "      <td>430</td>\n",
              "      <td>Tripureshwor</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Football and Cricket are the most popular spor...</td>\n",
              "      <td>In what part of Kathmandu is Dasarath Rangasal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87583</th>\n",
              "      <td>628</td>\n",
              "      <td>Chinese</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Football and Cricket are the most popular spor...</td>\n",
              "      <td>Who assisted Nepal in renovating Dasarath Rang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87584</th>\n",
              "      <td>54</td>\n",
              "      <td>17 182</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The total length of roads in Nepal is recorded...</td>\n",
              "      <td>As of 2004 how many kilometers of road existed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87585</th>\n",
              "      <td>289</td>\n",
              "      <td>hilly terrain</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The total length of roads in Nepal is recorded...</td>\n",
              "      <td>Why is travel in Kathmandu mainly via automobi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87586</th>\n",
              "      <td>500</td>\n",
              "      <td>BP</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The total length of roads in Nepal is recorded...</td>\n",
              "      <td>What highway connecting Kathmandu to elsewhere...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87587</th>\n",
              "      <td>457</td>\n",
              "      <td>west</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The total length of roads in Nepal is recorded...</td>\n",
              "      <td>In what direction out of Kathmandu does the Pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87588</th>\n",
              "      <td>466</td>\n",
              "      <td>Araniko</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The total length of roads in Nepal is recorded...</td>\n",
              "      <td>If one wished to travel north out of Kathmandu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87589</th>\n",
              "      <td>71</td>\n",
              "      <td>Tribhuvan International Airport</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The main international airport serving Kathman...</td>\n",
              "      <td>What is Nepal s primary airport for internatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87590</th>\n",
              "      <td>134</td>\n",
              "      <td>6</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The main international airport serving Kathman...</td>\n",
              "      <td>Starting in the center of Kathmandu how many k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87591</th>\n",
              "      <td>297</td>\n",
              "      <td>22</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The main international airport serving Kathman...</td>\n",
              "      <td>How many airlines use Tribhuvan International ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87592</th>\n",
              "      <td>698</td>\n",
              "      <td>Amsterdam</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The main international airport serving Kathman...</td>\n",
              "      <td>From what city does Arkefly offer nonstop flig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87593</th>\n",
              "      <td>734</td>\n",
              "      <td>Turkish Airlines</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The main international airport serving Kathman...</td>\n",
              "      <td>Who operates flights between Kathmandu and Ist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87594</th>\n",
              "      <td>229</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Kathmandu Metropolitan City KMC in order to pr...</td>\n",
              "      <td>In what US state did Kathmandu first establish...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87595</th>\n",
              "      <td>414</td>\n",
              "      <td>Rangoon</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Kathmandu Metropolitan City KMC in order to pr...</td>\n",
              "      <td>What was Yangon previously known as</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87596</th>\n",
              "      <td>476</td>\n",
              "      <td>Minsk</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Kathmandu Metropolitan City KMC in order to pr...</td>\n",
              "      <td>With what Belorussian city does Kathmandu have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87597</th>\n",
              "      <td>199</td>\n",
              "      <td>1975</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Kathmandu Metropolitan City KMC in order to pr...</td>\n",
              "      <td>In what year did Kathmandu create its initial ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87598</th>\n",
              "      <td>0</td>\n",
              "      <td>Kathmandu Metropolitan City</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Kathmandu Metropolitan City KMC in order to pr...</td>\n",
              "      <td>What is KMC an initialism of</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>87599 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       answer_start  ...                                           question\n",
              "0               515  ...  To whom did the Virgin Mary allegedly appear i...\n",
              "1               188  ...  What is in front of the Notre Dame Main Building \n",
              "2               279  ...  The Basilica of the Sacred heart at Notre Dame...\n",
              "3               381  ...                  What is the Grotto at Notre Dame \n",
              "4                92  ...  What sits on top of the Main Building at Notre...\n",
              "5               248  ...  When did the Scholastic Magazine of Notre dame...\n",
              "6               441  ...   How often is Notre Dame s the Juggler published \n",
              "7               598  ...  What is the daily student paper at Notre Dame ...\n",
              "8               126  ...  How many student news papers are found at Notr...\n",
              "9               908  ...  In what year did the student paper Common Sens...\n",
              "10              119  ...  Where is the headquarters of the Congregation ...\n",
              "11              145  ...  What is the primary seminary of the Congregati...\n",
              "12              234  ...        What is the oldest structure at Notre Dame \n",
              "13              356  ...  What individuals live at Fatima House at Notre...\n",
              "14              675  ...         Which prize did Frederick Buechner create \n",
              "15              487  ...  How many BS level degrees are offered in the C...\n",
              "16               46  ...  In what year was the College of Engineering at...\n",
              "17              126  ...  Before the creation of the College of Engineer...\n",
              "18              271  ...  How many departments are within the Stinson Re...\n",
              "19              155  ...  The College of Science began to offer civil en...\n",
              "20              496  ...  What entity provides help with the management ...\n",
              "21               68  ...  How many colleges for undergraduates are at No...\n",
              "22              155  ...  What was created at Notre Dame in 1962 to assi...\n",
              "23              647  ...  Which organization declared the First Year of ...\n",
              "24              358  ...  The granting of Doctorate degrees first occurr...\n",
              "25              624  ...                   What type of degree is an M Div \n",
              "26             1163  ...  Which program at Notre Dame offers a Master of...\n",
              "27               92  ...  In what year was a Master of Arts course first...\n",
              "28              757  ...  Which department at Notre Dame is the only one...\n",
              "29                4  ...  What institute at Notre Dame studies the reaso...\n",
              "...             ...  ...                                                ...\n",
              "87569           209  ...     Where can a temple of the Jain faith be found \n",
              "87570           355  ...  Kathmandu valley is home to about how many Bah...\n",
              "87571           427  ...  Where is the Baha i national office located in...\n",
              "87572           633  ...  About what percentage of the Nepali population...\n",
              "87573           728  ...  About how many Christian houses of worship exi...\n",
              "87574            46  ...  Of what university is the Institute of Medicin...\n",
              "87575           123  ...  In what part of Kathmandu is the Institute of ...\n",
              "87576           219  ...  When did the Institute of Medicine begin to of...\n",
              "87577           425  ...                         What does KUSMS stand for \n",
              "87578           377  ...  What institution of tertiary education is know...\n",
              "87579             0  ...  Along with cricket what sport is highly popula...\n",
              "87580           160  ...                What body oversees soccer in Nepal \n",
              "87581           498  ...  How many people can fit in Dasarath Rangasala ...\n",
              "87582           430  ...  In what part of Kathmandu is Dasarath Rangasal...\n",
              "87583           628  ...  Who assisted Nepal in renovating Dasarath Rang...\n",
              "87584            54  ...  As of 2004 how many kilometers of road existed...\n",
              "87585           289  ...  Why is travel in Kathmandu mainly via automobi...\n",
              "87586           500  ...  What highway connecting Kathmandu to elsewhere...\n",
              "87587           457  ...  In what direction out of Kathmandu does the Pr...\n",
              "87588           466  ...  If one wished to travel north out of Kathmandu...\n",
              "87589            71  ...  What is Nepal s primary airport for internatio...\n",
              "87590           134  ...  Starting in the center of Kathmandu how many k...\n",
              "87591           297  ...  How many airlines use Tribhuvan International ...\n",
              "87592           698  ...  From what city does Arkefly offer nonstop flig...\n",
              "87593           734  ...  Who operates flights between Kathmandu and Ist...\n",
              "87594           229  ...  In what US state did Kathmandu first establish...\n",
              "87595           414  ...               What was Yangon previously known as \n",
              "87596           476  ...  With what Belorussian city does Kathmandu have...\n",
              "87597           199  ...  In what year did Kathmandu create its initial ...\n",
              "87598             0  ...                      What is KMC an initialism of \n",
              "\n",
              "[87599 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaaUpdrs_S19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "trainingData[\"question\"] = trainingData[\"question\"].str.lower()\n",
        "trainingData[\"article_title\"] = trainingData[\"article_title\"].str.lower()\n",
        "trainingData[\"paragraph_context\"] = trainingData[\"paragraph_context\"].str.lower()\n",
        "trainingData[\"answer_text\"] = trainingData[\"answer_text\"].str.lower()\n",
        "trainingData[\"answer_start\"] = pd.to_numeric(trainingData[\"answer_start\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-l8jQ_mb-Lje",
        "colab_type": "code",
        "outputId": "edffa433-1852-4c0e-a403-4291b5a26c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "devData = readFile(\"dev-v1.1.json\")\n",
        "devData[\"question\"] = devData[\"question\"].str.lower()\n",
        "devData[\"article_title\"] = devData[\"article_title\"].str.lower()\n",
        "devData[\"paragraph_context\"] = devData[\"paragraph_context\"].str.lower()\n",
        "devData[\"answer_text\"] = devData[\"answer_text\"].str.lower()\n",
        "devData[\"answer_start\"] = pd.to_numeric(devData[\"answer_start\"])\n",
        "print(\"Finished loading dev data and lowering appropriate columns.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished loading dev data and lowering appropriate columns.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pkJ61xyU-LjU",
        "colab_type": "code",
        "outputId": "fcf9d172-5d16-42da-f6fd-4889f098fc7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from sys import getsizeof\n",
        "def vocabulary():\n",
        "  from sklearn.feature_extraction.text import CountVectorizer\n",
        "  data_frame = trainingData + devData\n",
        "  data_frame = data_frame.astype(\"str\")\n",
        "  phrases = []\n",
        "  for idx,row in data_frame.iterrows():\n",
        "    phrases.append(row[\"question\"])\n",
        "    phrases.append(row[\"paragraph_context\"])\n",
        "    phrases.append(row[\"article_title\"])\n",
        "  for string in phrases:\n",
        "    assert \"catalanswhat\" not in str(string)\n",
        "  words = CountVectorizer().fit(phrases).get_feature_names()\n",
        "  assert \"raisedin\" not in words\n",
        "  return words\n",
        "def vocabularySize():\n",
        "  return len(vocabulary())\n",
        "def summaryStatistics(series):\n",
        "    numberOfWords = series.apply(lambda x: len(str(x).split(\" \")))\n",
        "    averageNumberOfWords = sum(numberOfWords) / len(numberOfWords)\n",
        "    return \"average: \" + str(averageNumberOfWords) + \"maximum: \" + str(max(numberOfWords)) + \" minimum: \" +str(min(numberOfWords))\n",
        "print(\"Size of vocabulary: \", vocabularySize())\n",
        "print(\"Words in each question: \",summaryStatistics(trainingData[\"question\"]))\n",
        "print(\"Words in each article title: \",summaryStatistics(trainingData[\"article_title\"]))\n",
        "print(\"Words in each context: \",summaryStatistics(trainingData[\"paragraph_context\"]))\n",
        "print(\"Words in each answer: \",summaryStatistics(trainingData[\"answer_text\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of vocabulary:  53662\n",
            "Words in each question:  average: 11.217582392493066maximum: 41 minimum: 1\n",
            "Words in each article title:  average: 2.006621080149317maximum: 9 minimum: 1\n",
            "Words in each context:  average: 123.7916528727497maximum: 678 minimum: 21\n",
            "Words in each answer:  average: 3.356339684242971maximum: 43 minimum: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8IUZ203W4qi",
        "colab_type": "text"
      },
      "source": [
        "####Get maximum ** possible** answer start.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnvr_TCH-Lkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "max_possible_answer_start = int(pd.concat((trainingData[\"answer_text\"],devData[\"answer_text\"])).str.len().max())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40XPLi7DW_Jd",
        "colab_type": "text"
      },
      "source": [
        "####Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGXSmR5M-Lk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ecPo0Jetse39"
      },
      "source": [
        "### Integer encode text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GCN8otUGse38"
      },
      "source": [
        "Used for manual encoding of text into integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z19nkJI8iFmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strings = pd.concat((trainingData,devData)).drop(\"answer_start\",axis=1)\n",
        "strings = strings.values.flatten()\n",
        "textTokenizer = Tokenizer()\n",
        "textTokenizer.fit_on_texts(strings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iymfrGtadU0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get length of vocabulary.\n",
        "vocabulary_length = len(textTokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh5c5MeLe2Z1",
        "colab_type": "code",
        "outputId": "132fb7b2-a9d0-4c80-dbdc-33a6108704ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "# Get maximum length of all questions and contexts.\n",
        "max_length_questions = pd.concat((trainingData[\"question\"],devData[\"question\"])).str.split().len().max\n",
        "max_length_context = pd.concat((trainingData[\"paragraph_context\"],devData[\"paragraph_context\"])).str.len().max()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b7a943cfd50e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmax_length_questions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmax_length_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"paragraph_context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"paragraph_context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'len'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqbzwlpakw8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questionsTokenized_train = pad_sequences(np.array(textTokenizer.texts_to_sequences(trainingData[\"question\"])),maxlen=max_length_questions)\n",
        "contextTokenized_train = pad_sequences(np.array(textTokenizer.texts_to_sequences(trainingData[\"paragraph_context\"])),maxlen=max_length_context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXVcBRqidB6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questionsTokenized_dev = textTokenizer.texts_to_sequences(devData[\"question\"])\n",
        "contextTokenized_dev = textTokenizer.texts_to_sequences(devData[\"paragraph_context\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_ZjWkiReXH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad sequences.\n",
        "questionsTokenized_train = pad_sequences(questionsTokenized_train,maxlen=max_length_questions)\n",
        "contextTokenized_train = pad_sequences(contextTokenized_train,maxlen=max_length_context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oq_sLBsSse3_"
      },
      "source": [
        "## Second neural network - non-recurrent/no transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IniJzW3lse3-"
      },
      "source": [
        "This neural network is used to generate answers from the questions and articles. It works by first reading the relevant article and using the question to find the answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V7fyv6DQse3V",
        "outputId": "c65b5ae6-734c-402a-a8cc-becdc1bfef98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "#Build the neural network.\n",
        "from math import log\n",
        "inputShape_second = X_2_train_num.shape[1:3]\n",
        "print(inputShape_second)\n",
        "answers_shape = Y_2_train_num.shape\n",
        "print(answers_shape[1])\n",
        "# Find the vocabulary length.\n",
        "vocabularyLength = np.concatenate((X_2_train_num,X_2_dev_num)).max() + 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-056fa452021a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minputShape_second\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_2_train_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputShape_second\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0manswers_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_2_train_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswers_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_2_train_num' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Iv25QYbbse3Q",
        "outputId": "2b705537-b0fe-4934-ad3f-1ad0d6365054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "questions = Input(shape=(questionsTrain.shape[1],))\n",
        "context = Input(shape=(contextTrain.shape[1],))\n",
        "embedding_1 = Embedding(vocabularyLength,16)(questions)\n",
        "#answers_network.add(Dense(16))\n",
        "flatten_1 = Flatten()(embedding_1)\n",
        "hidden = Dense(16)(Dense(8)(flatten_1))\n",
        "hidden_2 = Dense(16)(hidden)\n",
        "hidden_3 = Dense(16)(hidden_2)\n",
        "dropout = Dropout(0.45)(hidden_3)\n",
        "output = Dense(answers_shape[1],activation='softmax')(dropout)\n",
        "answers_network = Model(inputs=[questions,context],outputs=output)\n",
        "answers_network.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7aff34f6a1d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestionsTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontextTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membedding_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabularyLength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#answers_network.add(Dense(16))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mflatten_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'questionsTrain' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eTLaKjwyse3N",
        "colab": {}
      },
      "source": [
        "answers_network.compile(\"adam\",\"binary_crossentropy\",metrics=[f1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OSJyJ3qxse3L"
      },
      "source": [
        "#### Train the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IekQsi2Yse3J",
        "colab": {}
      },
      "source": [
        "answers_network_checkpoint = ModelCheckpoint('answers_network-non-rnn-best.h5', verbose=1, monitor='val_f1',save_best_only=True, mode='auto') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_tWTvWxSse3F",
        "colab": {}
      },
      "source": [
        "print(answers_network.metrics_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9hg2J0qBse3C",
        "colab": {}
      },
      "source": [
        "\n",
        "answers_network.fit(x=[questionsTrain,contextTrain],y=Y_2_train_num,callbacks=[answers_network_checkpoint],validation_split=0.2,verbose=True,epochs=9)\n",
        "#print(\"Weights: \",questions_article_model.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LWDY4xhAse3A"
      },
      "source": [
        "#### Loading the model with best fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rEudSH2Ese29",
        "colab": {}
      },
      "source": [
        "answers_network.load_weights('answers_network-non-rnn-best.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ewsndbURse24",
        "colab": {}
      },
      "source": [
        "\n",
        "answers_network.evaluate([questionsDev,contextDev],Y_2_dev_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT2h3iAU-LlT",
        "colab_type": "text"
      },
      "source": [
        "## Transfer learning using Google's Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGUwjE61-LlU",
        "colab_type": "text"
      },
      "source": [
        "Our model has a very poor F1 score. Let's see if we can't build a better model. We'll use Google's BERT deep learning network, which is so good that the networks with SQuaD 2.0's highest recorded F1 score use it.  \n",
        "\n",
        "The basic idea behind BERT is this: Neural networks rely on numerical vectors. Similar sentences and phrases should produce similar vectors.\n",
        "\n",
        "\"It is not possible to train bidirectional models by simply conditioning each word on words before and after it. Doing this would allow the word that’s being predicted to indirectly see itself in a multi-layer model. To solve this, Google researchers used a straightforward technique of masking out some words in the input and condition each word bidirectionally in order to predict the masked words. This idea is not new, but BERT is the first technique where it was successfully used to pre-train a deep neural network.\" (packtpub.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJGqNFLxwpJJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "#### Downloading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfnMCv0uXSWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svwSEHh1-Llb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### The neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYxwvFi29eZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(currentDirectory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOPb0C6oWxJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "eb59bbbb-6c04-46ff-8eb1-51c95d41b89a"
      },
      "source": [
        "!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9SNLQU--Llc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
        "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
        "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
        "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tLYVr5bX8be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TF_KERAS must be added to environment variables in order to use TPU\n",
        "os.environ['TF_KERAS'] = '1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzoNvlduX9mS",
        "colab_type": "code",
        "outputId": "d83b02d5-198c-4a4b-b4fd-8cd2b6188b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# @title Load Basic Model\n",
        "import codecs\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "assert os.path.exists(pretrained_path), str(pretrained_path + \" does not exist\")\n",
        "token_dict = {}\n",
        "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)\n",
        "\n",
        "model = load_trained_model_from_checkpoint(config_path, checkpoint_path)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdDxlW5ZCTbQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4097
        },
        "outputId": "16c5c738-538c-4871-9601-77c411e6ab54"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 512, 768), ( 23440896    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 512, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 512, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 512, 768)     393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 512, 768)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 512, 768)     1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 512, 768)     0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 512, 768)     0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 512, 768)     0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
            "==================================================================================================\n",
            "Total params: 108,891,648\n",
            "Trainable params: 0\n",
            "Non-trainable params: 108,891,648\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEKhgXry-Lle",
        "colab_type": "code",
        "outputId": "c72d70b5-0a65-40a7-8b74-490b2c5eaa89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "\n",
        "answers_network.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 128)          0           input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 128)          0           input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1024)         132096      lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 1024)         132096      lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 1024)         0           dense_11[0][0]                   \n",
            "                                                                 dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 1024)         0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 240)          246000      dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 510,192\n",
            "Trainable params: 510,192\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDLyZ09mzjD5",
        "colab_type": "code",
        "outputId": "a27b91f7-9d9a-41c4-9123-4184decdcef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "answers_network.compile(\"rmsprop\",\"categorical_crossentropy\",metrics=[f1])\n",
        "answers_network_checkpoint = ModelCheckpoint('answers_network-rnn-best.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "with tf.Session() as session:\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  y=session.run(tf.one_hot(Y_2_train,max_answer_start+1))\n",
        "  print(X_2_train_text.shape)\n",
        "  print(y.shape)\n",
        "  x = np.hsplit(X_2_train_text,2)\n",
        "  answers_network.fit(x=x,y=y,callbacks=[answers_network_checkpoint],epochs=8,validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(87599, 2)\n",
            "(87599, 240)\n",
            "Train on 70079 samples, validate on 17520 samples\n",
            "Epoch 1/8\n",
            "70016/70079 [============================>.] - ETA: 0s - loss: 2.6111 - f1: 4.6314e-04\n",
            "Epoch 00001: val_loss improved from inf to 2.33924, saving model to answers_network-rnn-best.h5\n",
            "70079/70079 [==============================] - 39s 551us/sample - loss: 2.6113 - f1: 4.6272e-04 - val_loss: 2.3392 - val_f1: 0.0000e+00\n",
            "Epoch 2/8\n",
            "70048/70079 [============================>.] - ETA: 0s - loss: 2.5859 - f1: 0.0013\n",
            "Epoch 00002: val_loss did not improve from 2.33924\n",
            "70079/70079 [==============================] - 38s 535us/sample - loss: 2.5856 - f1: 0.0013 - val_loss: 2.3703 - val_f1: 0.0000e+00\n",
            "Epoch 3/8\n",
            "69952/70079 [============================>.] - ETA: 0s - loss: 2.5735 - f1: 8.0789e-04\n",
            "Epoch 00003: val_loss did not improve from 2.33924\n",
            "70079/70079 [==============================] - 39s 556us/sample - loss: 2.5742 - f1: 8.0641e-04 - val_loss: 2.3829 - val_f1: 4.9543e-04\n",
            "Epoch 4/8\n",
            "69984/70079 [============================>.] - ETA: 0s - loss: 2.5684 - f1: 0.0012\n",
            "Epoch 00004: val_loss did not improve from 2.33924\n",
            "70079/70079 [==============================] - 36s 521us/sample - loss: 2.5689 - f1: 0.0012 - val_loss: 2.3933 - val_f1: 0.0010\n",
            "Epoch 5/8\n",
            "70016/70079 [============================>.] - ETA: 0s - loss: 2.5665 - f1: 7.8677e-04\n",
            "Epoch 00005: val_loss did not improve from 2.33924\n",
            "70079/70079 [==============================] - 37s 525us/sample - loss: 2.5665 - f1: 7.8605e-04 - val_loss: 2.3944 - val_f1: 0.0010\n",
            "Epoch 6/8\n",
            "69952/70079 [============================>.] - ETA: 0s - loss: 2.5651 - f1: 6.1713e-04\n",
            "Epoch 00006: val_loss did not improve from 2.33924\n",
            "70079/70079 [==============================] - 37s 528us/sample - loss: 2.5655 - f1: 6.1600e-04 - val_loss: 2.3831 - val_f1: 0.0000e+00\n",
            "Epoch 7/8\n",
            "70048/70079 [============================>.] - ETA: 0s - loss: 2.5652 - f1: 1.6678e-04\n",
            "Epoch 00007: val_loss did not improve from 2.33924\n",
            "70079/70079 [==============================] - 37s 521us/sample - loss: 2.5654 - f1: 1.6670e-04 - val_loss: 2.3911 - val_f1: 0.0000e+00\n",
            "Epoch 8/8\n",
            "70016/70079 [============================>.] - ETA: 0s - loss: 2.5689 - f1: 2.3010e-04\n",
            "Epoch 00008: val_loss did not improve from 2.33924\n",
            "70079/70079 [==============================] - 36s 515us/sample - loss: 2.5687 - f1: 2.2989e-04 - val_loss: 2.3783 - val_f1: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}