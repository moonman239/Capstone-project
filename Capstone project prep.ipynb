{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Just to verify that everything's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Activation,InputLayer,Input,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from IPython.display import display\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import sys\n",
    "assert sys.version_info[0] >= 3\n",
    "print(\"Set up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing step, we will create two dictionaries. One will be used to map questions to articles, while the other will be used to map questions and the contents of the articles to the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading JSON datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainingData = pd.read_csv(\"training_data.csv\",dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading training data.\n",
      "Words in each question:  average: 11.29124835441589maximum: 60 minimum: 1\n",
      "Words in each article title:  average: 2.1835040924961366maximum: 10 minimum: 1\n",
      "Words in each context:  average: 137.88847804933891maximum: 766 minimum: 22\n",
      "Words in each answer:  average: 3.3740827657260604maximum: 46 minimum: 1\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "def summaryStatistics(series):\n",
    "    numberOfWords = series.apply(lambda x: len(str(x).split(\" \")))\n",
    "    averageNumberOfWords = sum(numberOfWords) / len(numberOfWords)\n",
    "    return \"average: \" + str(averageNumberOfWords) + \"maximum: \" + str(max(numberOfWords)) + \" minimum: \" +str(min(numberOfWords))\n",
    "print(\"Finished loading training data.\")\n",
    "print(\"Words in each question: \",summaryStatistics(trainingData[\"question\"]))\n",
    "print(\"Words in each article title: \",summaryStatistics(trainingData[\"title\"]))\n",
    "print(\"Words in each context: \",summaryStatistics(trainingData[\"context\"]))\n",
    "print(\"Words in each answer: \",summaryStatistics(trainingData[\"answer_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading dev data.\n",
      "Size of dev data:14.796387672424316 MB\n"
     ]
    }
   ],
   "source": [
    "devData = pd.read_csv(\"dev_data.csv\",dtype=object)\n",
    "print(\"Finished loading dev data.\")\n",
    "print(\"Size of dev data:\" + str(getsizeof(devData) / 1024**2) + \" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create new columns in the training and dev datasets for where in the context strings the answers end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training answer end column created.\n"
     ]
    }
   ],
   "source": [
    "sum_training = trainingData[\"answer_start\"].astype(\"int64\") + trainingData[\"answer_text\"].astype(\"str\").str.len()\n",
    "trainingData[\"answer_end\"] = sum_training\n",
    "print(\"Training answer end column created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev answer end column created.\n"
     ]
    }
   ],
   "source": [
    "sum_dev = devData[\"answer_start\"].astype(\"int64\") + devData[\"answer_text\"].astype(\"str\").str.len()\n",
    "devData[\"answer_end\"] = sum_dev\n",
    "print(\"Dev answer end column created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert answer_start to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted back to int64\n"
     ]
    }
   ],
   "source": [
    "trainingData[\"answer_start\"] = trainingData[\"answer_start\"].astype(\"int64\")\n",
    "devData[\"answer_start\"] = devData[\"answer_start\"].astype(\"int64\")\n",
    "print(\"Converted back to int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure there are no NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything's all good!\n"
     ]
    }
   ],
   "source": [
    "trainingNans = pd.concat((trainingData[\"answer_start\"].loc[trainingData[\"answer_start\"].isnull()],trainingData[\"answer_end\"].loc[trainingData[\"answer_end\"].isnull()]))\n",
    "assert trainingNans.shape[0] == 0,\"NANs detected in training set: \" + str(trainingNans)\n",
    "devNans = pd.concat((devData[\"answer_start\"].loc[devData[\"answer_start\"].isnull()],devData[\"answer_end\"].loc[devData[\"answer_end\"].isnull()]))\n",
    "assert devNans.shape[0] == 0, \"NANs detected in dev set: \" + str(devNans)\n",
    "print(\"Everything's all good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text preprocessing using scikit-learn and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section turns inputs into n-grams. Note that order is not necessarily preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Custom skip-gram vectorizer\n",
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "    \n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        \"\"\"Turn tokens into a sequence of 1-skip-2-grams after stop words filtering\"\"\"\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vectorize(trainData,devData):\n",
    "    vectorizer = SkipGramVectorizer()\n",
    "    combinedData = pd.concat((trainData,devData))\n",
    "    vectorizer.fit(combinedData)\n",
    "    print(\"Vectorizer fit.\")\n",
    "    trainData_vectorized = vectorizer.transform(trainData)\n",
    "    print(\"First text vectorized\")\n",
    "    devData_vectorized = vectorizer.transform(devData)\n",
    "    print(\"Second text vectorized\")\n",
    "    return trainData_vectorized,devData_vectorized,vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer fit.\n",
      "First text vectorized\n",
      "Second text vectorized\n",
      "Shape: (87355, 370435)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vectorize all questions.\n",
    "training_questions,dev_questions,questionsVocabulary = vectorize(trainingData[\"question\"],devData[\"question\"])\n",
    "print(\"Shape:\",training_questions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of keys 370435\n",
      "['knots it', 'provisions the', 'about electron', 'antibiotics food', 'in earn', 'composer producer', 'are per', 'veneration images', 'lower music', 'have postoperative']\n"
     ]
    }
   ],
   "source": [
    "keys = list(questionsVocabulary.keys())\n",
    "print(\"# of keys\", len(keys))\n",
    "print(keys[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article titles binarized\n"
     ]
    }
   ],
   "source": [
    "# Training set.\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "combinedArticleTitles = pd.concat((trainingData[\"title\"],devData[\"title\"]))\n",
    "binarizer = MultiLabelBinarizer().fit(combinedArticleTitles.str.split(\" \"))\n",
    "print(\"Article titles binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  761\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes: \",len(binarizer.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize article content (including answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered content; fitting binarizer\n",
      "Binarizer fit. Transforming data.\n",
      "Data transformed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "contextTotal = pd.concat((trainingData[\"context\"],devData[\"context\"]))\n",
    "print(\"Gathered content; fitting binarizer\")\n",
    "contextBinarizer = MultiLabelBinarizer().fit(contextTotal)\n",
    "print(\"Binarizer fit. Transforming data.\")\n",
    "trainingContextBinarized = contextBinarizer.transform(trainingData[\"context\"])\n",
    "devContextBinarized = contextBinarizer.transform(devData[\"context\"])\n",
    "print(\"Data transformed.\")\n",
    "#print(\"Content vectorizer fit.\")\n",
    "#print(\"Vocabulary: \" + str(articleContentVocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes 1420\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes\",len(contextBinarizer.classes_))\n",
    "#print(pd.Series(contextBinarizer.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarize the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering answers and formatting them.\n",
      "Processed answers. Binarizing answers\n",
      "Finished binarizing answers.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\"Gathering answers and formatting them.\")\n",
    "print(\"Processed answers. Binarizing answers\")\n",
    "# Convert answer_start and answer_end columns to equal-width arrays containing ones and zeros.\n",
    "# Get the highest possible number across the two columns.\n",
    "highestNumber = pd.concat((trainingData[\"answer_end\"],devData[\"answer_end\"])).max()\n",
    "binaryNumberWidth = int(math.log(highestNumber,2)) + 1\n",
    "trainingAnswerStartsBinarized = trainingData[\"answer_start\"].map(lambda x: [int(y) for y in np.binary_repr(int(x),width=binaryNumberWidth)])\n",
    "trainingAnswerEndsBinarized = trainingData[\"answer_end\"].map(lambda x: [int(y) for y in np.binary_repr(int(x),width=binaryNumberWidth)])\n",
    "devAnswerStartsBinarized = devData[\"answer_start\"].map(lambda x: [int(y) for y in np.binary_repr(int(x),width=binaryNumberWidth)])\n",
    "devAnswerEndsBinarized = devData[\"answer_end\"].map(lambda x: [int(y) for y in np.binary_repr(int(x),width=binaryNumberWidth)])\n",
    "#trainingAnswerEndsBinarized = pd.get_dummies(trainingAnswers[\"answer_end\"])\n",
    "#devAnswerStartsBinarized = pd.get_dummies(devAnswers[\"answer_start\"])\n",
    "#devAnswerEndsBinarized = pd.get_dummies(devAnswers[\"answer_end\"])\n",
    "print(\"Finished binarizing answers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into testing and cross-validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 20% of data will be set aside for cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_1_train,X_1_cross_validation,Y_1_train,Y_1_cross_validation = train_test_split(training_questions,training_articleTitles)\n",
    "print(type(X_1_train))\n",
    "print(X_1_train.shape)\n",
    "print(Y_1_train.shape)\n",
    "print(X_1_cross_validation.shape)\n",
    "print(Y_1_cross_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "X_2 = hstack((training_questions,training_articleTitles))\n",
    "print(\"X 2 horizontally stacked.\")\n",
    "X_2_train,X_2_cross_validation,Y_2_train,Y_2_cross_validation = train_test_split(X_2,training_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_2_dev = hstack((dev_questions,dev_articleTitles))\n",
    "print(\"X 2 dev horizontally stacked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed datasets to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz(\"X_1_train\",X_1_train)\n",
    "save_npz(\"X_1_cross_validation\",X_1_cross_validation)\n",
    "save_npz(\"dev_questions\",dev_questions)\n",
    "save_npz(\"X_2_train\",X_2_train)\n",
    "save_npz(\"X_2_cross_validation\",X_2_cross_validation)\n",
    "save_npz(\"X_2_dev\",X_2_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz(\"Y_1_train\",Y_1_train)\n",
    "save_npz(\"Y_1_cross_validation\",Y_1_cross_validation)\n",
    "save_npz(\"dev_articleTitles\",dev_articleTitles)\n",
    "save_npz(\"Y_2_train\",csr_matrix(Y_2_train))\n",
    "save_npz(\"Y_2_cross_validation\",csr_matrix(Y_2_cross_validation))\n",
    "save_npz(\"dev_answers\",csr_matrix(dev_answers))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-preprocessing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get vocabularies from preprocessors above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
