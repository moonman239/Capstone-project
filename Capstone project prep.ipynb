{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Just to verify that everything's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Activation,InputLayer,Input,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from IPython.display import display\n",
    "# Verify we are using GPU.\n",
    "from tensorflow.python.client import device_lib\n",
    "import sys\n",
    "print(sys.maxsize)\n",
    "assert sys.version_info[0] >= 3\n",
    "print(\"Set up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing step, we will create two dictionaries. One will be used to map questions to articles, while the other will be used to map questions and the contents of the articles to the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading JSON datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = pd.read_csv(\"training_data.csv\")\n",
    "display(trainingData)\n",
    "from sys import getsizeof\n",
    "print(\"Finished loading training data.\")\n",
    "print(\"Size of training data:\" + str(getsizeof(trainingData) / 1024**2) + \" MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devData = pd.read_csv(\"dev_data.csv\")\n",
    "print(\"Finished loading dev data.\")\n",
    "print(\"Size of dev data:\" + str(getsizeof(devData) / 1024**2) + \" MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training/testing data\n",
    "trainingData.to_csv(\"training_data\")\n",
    "devData.to_csv(\"dev_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing using scikit-learn and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section turns inputs into n-grams. Note that order is not necessarily preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom skip-gram vectorizer\n",
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "    \n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        \"\"\"Turn tokens into a sequence of 1-skip-2-grams after stop words filtering\"\"\"\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(trainData,devData):\n",
    "    vectorizer = SkipGramVectorizer()\n",
    "    combinedData = pd.concat((trainData,devData))\n",
    "    vectorizer.fit(combinedData)\n",
    "    print(\"Vectorizer fit.\")\n",
    "    trainData_vectorized = vectorizer.transform(trainData)\n",
    "    print(\"First text vectorized\")\n",
    "    devData_vectorized = vectorizer.transform(devData)\n",
    "    print(\"Second text vectorized\")\n",
    "    return trainData_vectorized,devData_vectorized,vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Vectorize all questions.\n",
    "training_questions,dev_questions,vocabulary1 = vectorize(trainingData[\"question\"],devData[\"question\"])\n",
    "print(\"Number of features:\",training_questions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_split = lambda series: series.apply(lambda x: x.split(\" \")) # Splits a Pandas series of strings into words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set.\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "combinedArticleTitles = series_split(pd.concat((trainingData[\"title\"],devData[\"title\"])))\n",
    "articleTitlesBinarizer = MultiLabelBinarizer().fit(combinedArticleTitles)\n",
    "print(\"Article titles binarized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace numbers with text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "\n",
    "def convertOneNumberToWord(word):\n",
    "    try:\n",
    "        word = int(word)\n",
    "    except ValueError:\n",
    "        return word\n",
    "    return num2words(word)\n",
    "def n2w(string):\n",
    "    splitString = string.split(\" \")\n",
    "    conversion = []\n",
    "    for word in splitString:\n",
    "        conversion.append(convertOneNumberToWord(word))\n",
    "    return \" \".join(conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code.\n",
    "print(n2w(\"I have 2 tomatoes.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData[\"context\"] = trainingData[\"context\"].apply(n2w)\n",
    "devData[\"context\"] = devData[\"context\"].apply(n2w)\n",
    "print(\"Successfully converted numbers to words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize article content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contentTotal = pd.concat((trainingData[\"context\"],devData[\"context\"]))\n",
    "print(\"Gathered content\")\n",
    "contentTotal = series_split(contentTotal)\n",
    "contentBinarizer = MultiLabelBinarizer()\n",
    "contentBinarizer.fit(contentTotal)\n",
    "test = contentBinarizer.inverse_transform(contentBinarizer.transform(series_split(pd.Series(\"Saint Bernadette\"))))\n",
    "print(\"Test\",test)\n",
    "print(\"Content binarizer fit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_context = contentBinarizer.transform(series_split(trainingData[\"context\"]))\n",
    "print(\"Training context binarized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_context = contentBinarizer.transform(series_split(devData[\"context\"]))\n",
    "print(\"Dev context binarized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Num2words on answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData[\"answer_text\"] = trainingData[\"answer_text\"].apply(n2w)\n",
    "print(\"Converted training answers text numbers to words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devData[\"answer_text\"] = devData[\"answer_text\"].apply(n2w)\n",
    "print(\"Converted dev answers text numbers to words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarize the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findErrorSentences(series):\n",
    "    sentences = series_split(series)\n",
    "    vocabulary = contentBinarizer.classes_\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in vocabulary:\n",
    "                print(\"Found an error sentence!\")\n",
    "                print(\" \".join(sentence))\n",
    "    return error_sentences\n",
    "findErrorSentences(trainingData[\"answer_text\"])\n",
    "print(\"Error sentences: \" + str(error_sentences))\n",
    "training_answers = contentBinarizer.transform(series_split(trainingData[\"answer_text\"]))\n",
    "print(\"Training answers binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_answers = contentBinarizer.transform(series_split(devData[\"answer_text\"]))\n",
    "print(\"Dev answers binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features for questions vectorizer:\",len(vocabulary1))\n",
    "print(\"Number of features in article titles:\",len(articleTitles_vocabulary))\n",
    "print(\"Number of features in context:\",len(contentBinarizer.classes_))\n",
    "print(\"Number of features in answers: \",len(answersBinarizer.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've vectorized the questions and binarized the article context, we have one problem. First, the questions are represented by ordinal numbers. Our neural network will work best if the questions are represented as binary numbers; i.e, they contain only ones and zeroes. Second, each question in the dataset has 371,624 features. The dataset itself contains approximately 90,000 entries. This is a common problem in data science; having more features than data can lead the algorithm to miss significant relationships between the features and the end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "training_questions = normalize(training_questions)\n",
    "dev_questions = normalize(dev_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to numpy arrays, get input shape\n",
    "inputShape_first = training_questions.toarray().shape[1:] # Input shape of the first neural network.\n",
    "article_titles_shape = training_articleTitles.toarray().shape\n",
    "print(\"First neural network X shape: \" + str(training_questions.shape))\n",
    "print(\"First neural network input shape: \" + str(inputShape_first))\n",
    "print(\"Article titles array shape: \" + str(article_titles_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into testing and cross-validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% of data will be set aside for cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_1_train,X_1_cross_validation,Y_1_train,Y_1_cross_validation = train_test_split(training_questions,training_articleTitles)\n",
    "print(type(X_1_train))\n",
    "print(X_1_train.shape)\n",
    "print(Y_1_train.shape)\n",
    "print(X_1_cross_validation.shape)\n",
    "print(Y_1_cross_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "X_2 = hstack((training_questions,training_articleTitles))\n",
    "print(\"X 2 horizontally stacked.\")\n",
    "X_2_train,X_2_cross_validation,Y_2_train,Y_2_cross_validation = train_test_split(X_2,training_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2_dev = hstack((dev_questions,dev_articleTitles))\n",
    "print(\"X 2 dev horizontally stacked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed datasets to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz(\"X_1_train\",X_1_train)\n",
    "save_npz(\"X_1_cross_validation\",X_1_cross_validation)\n",
    "save_npz(\"dev_questions\",dev_questions)\n",
    "save_npz(\"X_2_train\",X_2_train)\n",
    "save_npz(\"X_2_cross_validation\",X_2_cross_validation)\n",
    "save_npz(\"X_2_dev\",X_2_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz(\"Y_1_train\",Y_1_train)\n",
    "save_npz(\"Y_1_cross_validation\",Y_1_cross_validation)\n",
    "save_npz(\"dev_articleTitles\",dev_articleTitles)\n",
    "save_npz(\"Y_2_train\",csr_matrix(Y_2_train))\n",
    "save_npz(\"Y_2_cross_validation\",csr_matrix(Y_2_cross_validation))\n",
    "save_npz(\"dev_answers\",csr_matrix(dev_answers))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-preprocessing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabularies from preprocessors above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
