{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Just to verify that everything's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Activation,InputLayer,Input,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Verify we are using GPU.\n",
    "from tensorflow.python.client import device_lib\n",
    "import sys\n",
    "print(sys.maxsize)\n",
    "assert sys.version_info[0] >= 3\n",
    "print(\"Set up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing step, we will create two dictionaries. One will be used to map questions to articles, while the other will be used to map questions and the contents of the articles to the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading JSON datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('[1-9]*[1-9]')\n",
    "def n2w(_string):\n",
    "    isInt = True\n",
    "    stringToReturn = \"\"\n",
    "    try:\n",
    "        stringToReturn = num2words(int(_string))\n",
    "    except:\n",
    "        stringToReturn = _string\n",
    "        #assert isinstance(stringToReturn,str)\n",
    "        return stringToReturn\n",
    "def convertNumbersToWords(_string):\n",
    "    #Error: expected string?\n",
    "    #assert isinstance(_string,str)\n",
    "    _string_copy = p.sub(lambda x: n2w(x.group()),_string)\n",
    "    return _string_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "def readFile(filename):\n",
    "    with open(filename) as file:\n",
    "        data = json.load(file)[\"data\"]\n",
    "        paragraphs = pd.io.json.json_normalize(data,record_path=\"paragraphs\")\n",
    "        # Map paragraph IDs and qas IDs.\n",
    "        paragraphIDs_questions = []\n",
    "        lastParagraphId = 0\n",
    "        for index,paragraph in paragraphs.iterrows():\n",
    "            for qasElement in paragraph[\"qas\"]:\n",
    "                paragraphIDs_questions.append({\"paragraphID\":lastParagraphId,\"question\":qasElement[\"question\"]})\n",
    "            lastParagraphId = lastParagraphId + 1\n",
    "        paragraphIDs_questions = pd.DataFrame(paragraphIDs_questions)\n",
    "        print(\"Finished mapping paragraph IDs to qas questions. Now generating qas dataframe and doing other things.\")\n",
    "        qas = pd.io.json.json_normalize(data,record_path=[\"paragraphs\",\"qas\"],meta=[\"title\"])\n",
    "        paragraphs[\"id\"] = paragraphs.index\n",
    "        #print(qas[\"question\"])\n",
    "        #Gather a list of where all answers should be so we can shove them into a DataFrame.\n",
    "        # Haven't found a more efficient way to do this yet.\n",
    "        answer_ids = set()\n",
    "        answerId = 0\n",
    "        for index,row in qas.iterrows():\n",
    "            answer_ids.add(answerId)\n",
    "            answerId = answerId + len(row[\"answers\"])\n",
    "        print(\"Finished with answer ids.\")\n",
    "        # Map qas pair IDs to answer IDs.\n",
    "        answer_ids = pd.DataFrame(list(answer_ids))\n",
    "        print(\"Finished converting answer_ids to DataFrame.\")\n",
    "        question_answerId = pd.DataFrame(qas[\"question\"]).join(answer_ids,how=\"outer\")\n",
    "        question_answerId.columns = [\"question\",\"answer_id\"]\n",
    "        #print(\"Id-answerID columns: \",id_answerId.columns)\n",
    "        print(\"finished creating intermediary table.\")\n",
    "        # Load answers into a data frame.\n",
    "        answers = pd.io.json.json_normalize(data,record_path=[\"paragraphs\",\"qas\",\"answers\"])\n",
    "        answers.rename(columns={\"text\":\"answer_text\"},inplace=True)\n",
    "        # Give each answer an ID.\n",
    "        answers[\"id\"] = answers.index\n",
    "        print(\"Finished creating answers dataframe.\")\n",
    "        qas = qas.drop(labels=[\"answers\"],axis=1) # Not needed any longer; we have the answers!\n",
    "        #print(\"Dropped column 'answers' from qas.\")\n",
    "        # Map qas dataframe to answer table via id_answerId\n",
    "        qas_answerId = pd.merge(qas,question_answerId,how=\"inner\",on=\"question\")\n",
    "        # Check that no duplicates exist in qas_answerId\n",
    "        qas_answerId = qas_answerId.drop_duplicates(\"question\")\n",
    "        assert qas_answerId.duplicated(\"question\").any() == False\n",
    "        print(\"Finished joining qas to answer id\")\n",
    "        # Merge qas_answerId with answers.\n",
    "        qas = pd.merge(qas_answerId,answers,how=\"inner\",left_on=\"answer_id\",right_on=\"id\")\n",
    "        #print(\"Returned data frame: \",returnDataFrame)\n",
    "        # Finally, include context.\n",
    "        context = paragraphs[[\"context\",\"id\"]]\n",
    "        qasWithParagraphID = pd.merge(qas,paragraphIDs_questions,how=\"inner\",on=\"question\")\n",
    "        qas = pd.merge(qasWithParagraphID,context,how=\"inner\",left_on=\"paragraphID\",right_on=\"id\")\n",
    "        print(\"Done!\")\n",
    "        return qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished mapping paragraph IDs to qas questions. Now generating qas dataframe and doing other things.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-198ecd5bf638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train-v1.1.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"When did Beyonc√© release her first solo album?\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-85076934d31b>\u001b[0m in \u001b[0;36mreadFile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mparagraphIDs_questions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraphIDs_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished mapping paragraph IDs to qas questions. Now generating qas dataframe and doing other things.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mqas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecord_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"paragraphs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"qas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mparagraphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparagraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(qas[\"question\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/json/normalize.py\u001b[0m in \u001b[0;36mjson_normalize\u001b[0;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mrecords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0m_recursive_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/json/normalize.py\u001b[0m in \u001b[0;36m_recursive_extract\u001b[0;34m(data, path, seen_meta, level)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 _recursive_extract(obj[path[0]], path[1:],\n\u001b[0;32m--> 229\u001b[0;31m                                    seen_meta, level=level + 1)\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/json/normalize.py\u001b[0m in \u001b[0;36m_recursive_extract\u001b[0;34m(data, path, seen_meta, level)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mrecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pull_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;31m# For repeating the metadata later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/json/normalize.py\u001b[0m in \u001b[0;36m_pull_field\u001b[0;34m(js, spec)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pull_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test case.\n",
    "dataFrame = readFile(\"train-v1.1.json\")\n",
    "print (dataFrame.loc[dataFrame[\"question\"] == \"When did Beyonc√© release her first solo album?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished mapping paragraph IDs to qas questions. Now generating qas dataframe and doing other things.\n",
      "Finished with answer ids.\n",
      "Finished converting answer_ids to DataFrame.\n",
      "finished creating intermediary table.\n",
      "Finished creating answers dataframe.\n",
      "Finished joining qas to answer id\n",
      "Done!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6c7647349e92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mqas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mdataFrameContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mparagraph_context\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mdataFrameContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparagraph_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Context not matching!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_qas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_qas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not DataFrame"
     ]
    }
   ],
   "source": [
    "# Test that the above code works.\n",
    "import json\n",
    "dataFrame = readFile(\"train-v1.1.json\")\n",
    "#print(dataFrame)\n",
    "with open(\"train-v1.1.json\") as file:\n",
    "    data = json.load(file)[\"data\"]\n",
    "    for article in data:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            paragraph_context = paragraph[\"context\"]\n",
    "            qas = paragraph[\"qas\"]\n",
    "            dataFrameContext = dataFrame[dataFrame.context == paragraph_context]\n",
    "            assert dataFrameContext in paragraph_context, \"Context not matching!\"\n",
    "            for _qas in qas:\n",
    "                question = _qas[\"question\"]\n",
    "                answers = [answer[\"text\"] for answer in _qas[\"answers\"]]\n",
    "                dataFrameAnswers = dataFrame[dataFrame.question == question][\"answer_text\"].values\n",
    "                # If the above function works, the dataframe should contain the same question/answer pair.\n",
    "                assert dataFrameAnswers.shape[0] == 1, \"Assertion failed. Question:\" + question + \" answers: \" + dataFrameAnswers\n",
    "                #message = \"data frame answered \" + dataFrameAnswers + \"; actual answer is \" + answer + \" question: \" + question\n",
    "                assert type(answers[0]) == type(dataFrameAnswers[0])\n",
    "                assert str(dataFrameAnswers[0]) in answers, \"dataframe answer: \" + str(dataFrameAnswers) + \" actual answers:\" + str(answers)\n",
    "print(\"All good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = readFile(\"train-v1.1.json\")\n",
    "print(trainingData)\n",
    "from sys import getsizeof\n",
    "print(\"Finished loading training data.\")\n",
    "print(\"Size of training data:\",getsizeof(trainingData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Amount of training data: \",len(training_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_questions,dev_articleTitles,dev_articleTexts,dev_answers = readFile(\"dev-v1.1.json\")\n",
    "print(\"Finished loading dev data.\")\n",
    "print(\"Size of dev questions\",getsizeof(dev_questions) / 1024)\n",
    "print(\"Size of dev titles\",getsizeof(dev_articleTitles) / 1024)\n",
    "print(\"Size of dev texts\",getsizeof(dev_articleTexts) / 1024)\n",
    "print(\"Size of dev answers\",getsizeof(dev_answers) / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dev_questions) == len(dev_answers)\n",
    "print(\"Amount of dev data: \" + str(len(dev_questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing using scikit-learn and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section turns inputs into n-grams. Note that order is not necessarily preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom skip-gram vectorizer\n",
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "    \n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        \"\"\"Turn tokens into a sequence of 1-skip-2-grams after stop words filtering\"\"\"\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(trainData,devData):\n",
    "    vectorizer = SkipGramVectorizer()\n",
    "    vectorizer.fit(trainData + devData)\n",
    "    print(\"Vectorizer fit.\")\n",
    "    trainData_vectorized = vectorizer.transform(trainData)\n",
    "    print(\"First text vectorized\")\n",
    "    devData_vectorized = vectorizer.transform(devData)\n",
    "    print(\"Second text vectorized\")\n",
    "    return trainData_vectorized,devData_vectorized,vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Vectorize all questions.\n",
    "training_questions,dev_questions,question_vocabulary = vectorize(training_questions,dev_questions)\n",
    "print(\"Number of features:\",training_questions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set.\n",
    "training_articleTitles,dev_articleTitles,articleTitles_vocabulary = vectorize(training_articleTitles,dev_articleTitles)\n",
    "print(\"Article titles vectorized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize article content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "contentBinarizer = MultiLabelBinarizer()\n",
    "print(getsizeof(contentBinarizer))\n",
    "contentTotal = training_articleTexts + dev_articleTexts\n",
    "print(\"Gathered content\")\n",
    "contentBinarizer.fit(contentTotal)\n",
    "print(\"Content binarizer fit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_answers = contentBinarizer.transform(training_answers)\n",
    "print(\"Training answers binarized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_answers = contentBinarizer.transform(dev_answers)\n",
    "print(\"Dev answers binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features in questions:\",training_questions.shape[1])\n",
    "print(\"Number of features in answers:\",training_answers.shape[1])\n",
    "print(\"Number of features in article titles:\",training_articleTitles.shape[1])\n",
    "print(\"Number of features in content:\",training_articleContent.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to numpy arrays, get input shape\n",
    "inputShape_first = training_questions.toarray().shape[1:] # Input shape of the first neural network.\n",
    "article_titles_shape = training_articleTitles.toarray().shape\n",
    "print(\"First neural network X shape: \" + str(training_questions.shape))\n",
    "print(\"First neural network input shape: \" + str(inputShape_first))\n",
    "print(\"Article titles array shape: \" + str(article_titles_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into testing and cross-validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% of data will be set aside for cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_1_train,X_1_cross_validation,Y_1_train,Y_1_cross_validation = train_test_split(training_questions,training_articleTitles)\n",
    "print(type(X_1_train))\n",
    "print(X_1_train.shape)\n",
    "print(Y_1_train.shape)\n",
    "print(X_1_cross_validation.shape)\n",
    "print(Y_1_cross_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "X_2 = hstack((training_questions,training_articleTitles))\n",
    "print(\"X 2 horizontally stacked.\")\n",
    "X_2_train,X_2_cross_validation,Y_2_train,Y_2_cross_validation = train_test_split(X_2,training_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2_dev = hstack((dev_questions,dev_articleTitles))\n",
    "print(\"X 2 dev horizontally stacked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed datasets to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz(\"X_1_train\",X_1_train)\n",
    "save_npz(\"X_1_cross_validation\",X_1_cross_validation)\n",
    "save_npz(\"dev_questions\",dev_questions)\n",
    "save_npz(\"X_2_train\",X_2_train)\n",
    "save_npz(\"X_2_cross_validation\",X_2_cross_validation)\n",
    "save_npz(\"X_2_dev\",X_2_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz(\"Y_1_train\",Y_1_train)\n",
    "save_npz(\"Y_1_cross_validation\",Y_1_cross_validation)\n",
    "save_npz(\"dev_articleTitles\",dev_articleTitles)\n",
    "save_npz(\"Y_2_train\",csr_matrix(Y_2_train))\n",
    "save_npz(\"Y_2_cross_validation\",csr_matrix(Y_2_cross_validation))\n",
    "save_npz(\"dev_answers\",csr_matrix(dev_answers))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-preprocessing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabularies from preprocessors above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
