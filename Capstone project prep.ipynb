{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Just to verify that everything's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up!\n",
      "Set up!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Activation,InputLayer,Input,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from IPython.display import display\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import sys\n",
    "assert sys.version_info[0] >= 3\n",
    "print(\"Set up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing step, we will create two dictionaries. One will be used to map questions to articles, while the other will be used to map questions and the contents of the articles to the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading JSON datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainingData = pd.read_csv(\"training_data.csv\",dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading training data.\n",
      "Words in each question:  average: 11.29124835441589maximum: 60 minimum: 1\n",
      "Finished loading training data.\n",
      "Words in each question:  average: 11.29124835441589maximum: 60 minimum: 1\n",
      "Words in each article title:  average: 2.1835040924961366maximum: 10 minimum: 1\n",
      "Words in each article title:  average: 2.1835040924961366maximum: 10 minimum: 1\n",
      "Words in each context:  average: 137.88847804933891maximum: 766 minimum: 22\n",
      "Words in each answer:  average: 3.3740827657260604maximum: 46 minimum: 1\n",
      "Words in each context:  average: 137.88847804933891maximum: 766 minimum: 22\n",
      "Words in each answer:  average: 3.3740827657260604maximum: 46 minimum: 1\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "def summaryStatistics(series):\n",
    "    numberOfWords = series.apply(lambda x: len(str(x).split(\" \")))\n",
    "    averageNumberOfWords = sum(numberOfWords) / len(numberOfWords)\n",
    "    return \"average: \" + str(averageNumberOfWords) + \"maximum: \" + str(max(numberOfWords)) + \" minimum: \" +str(min(numberOfWords))\n",
    "print(\"Finished loading training data.\")\n",
    "print(\"Words in each question: \",summaryStatistics(trainingData[\"question\"]))\n",
    "print(\"Words in each article title: \",summaryStatistics(trainingData[\"title\"]))\n",
    "print(\"Words in each context: \",summaryStatistics(trainingData[\"context\"]))\n",
    "print(\"Words in each answer: \",summaryStatistics(trainingData[\"answer_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading dev data.\n",
      "Finished loading dev data.\n",
      "Size of dev data:14.796387672424316 MB\n",
      "Size of dev data:14.796387672424316 MB\n"
     ]
    }
   ],
   "source": [
    "devData = pd.read_csv(\"dev_data.csv\",dtype=object)\n",
    "print(\"Finished loading dev data.\")\n",
    "print(\"Size of dev data:\" + str(getsizeof(devData) / 1024**2) + \" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create new columns in the training and dev datasets for where in the context strings the answers end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training answer end column created.\n",
      "Training answer end column created.\n"
     ]
    }
   ],
   "source": [
    "sum_training = trainingData[\"answer_start\"].astype(\"int64\") + trainingData[\"answer_text\"].astype(\"str\").str.len()\n",
    "trainingData[\"answer_end\"] = sum_training\n",
    "print(\"Training answer end column created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev answer end column created.\n",
      "Dev answer end column created.\n"
     ]
    }
   ],
   "source": [
    "sum_dev = devData[\"answer_start\"].astype(\"int64\") + devData[\"answer_text\"].astype(\"str\").str.len()\n",
    "devData[\"answer_end\"] = sum_dev\n",
    "print(\"Dev answer end column created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert answer_start to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted back to int64\n",
      "Converted back to int64\n"
     ]
    }
   ],
   "source": [
    "trainingData[\"answer_start\"] = trainingData[\"answer_start\"].astype(\"int64\")\n",
    "devData[\"answer_start\"] = devData[\"answer_start\"].astype(\"int64\")\n",
    "print(\"Converted back to int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure there are no NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything's all good!\n",
      "Everything's all good!\n"
     ]
    }
   ],
   "source": [
    "trainingNans = pd.concat((trainingData[\"answer_start\"].loc[trainingData[\"answer_start\"].isnull()],trainingData[\"answer_end\"].loc[trainingData[\"answer_end\"].isnull()]))\n",
    "assert trainingNans.shape[0] == 0,\"NANs detected in training set: \" + str(trainingNans)\n",
    "devNans = pd.concat((devData[\"answer_start\"].loc[devData[\"answer_start\"].isnull()],devData[\"answer_end\"].loc[devData[\"answer_end\"].isnull()]))\n",
    "assert devNans.shape[0] == 0, \"NANs detected in dev set: \" + str(devNans)\n",
    "print(\"Everything's all good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text preprocessing using scikit-learn and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section turns inputs into n-grams. Note that order is not necessarily preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Custom skip-gram vectorizer\n",
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "    \n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        \"\"\"Turn tokens into a sequence of 1-skip-2-grams after stop words filtering\"\"\"\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vectorize(trainData,devData):\n",
    "    vectorizer = SkipGramVectorizer()\n",
    "    combinedData = pd.concat((trainData,devData))\n",
    "    vectorizer.fit(combinedData)\n",
    "    print(\"Vectorizer fit.\")\n",
    "    trainData_vectorized = vectorizer.transform(trainData)\n",
    "    print(\"First text vectorized\")\n",
    "    devData_vectorized = vectorizer.transform(devData)\n",
    "    print(\"Second text vectorized\")\n",
    "    return trainData_vectorized,devData_vectorized,vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer fit.\n",
      "Vectorizer fit.\n",
      "First text vectorized\n",
      "First text vectorized\n",
      "Second text vectorized\n",
      "Shape: (87355, 370435)\n",
      "Second text vectorized\n",
      "Shape: (87355, 370435)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vectorize all questions.\n",
    "training_questions,dev_questions,questionsVocabulary = vectorize(trainingData[\"question\"],devData[\"question\"])\n",
    "print(\"Shape:\",training_questions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of keys 370435\n",
      "['haven firefighters', 'in course', 'nikolic found', 'what respiratory', 'royal educated', 'constructed bitumont', 'nbc secure', 'what awarded', 'employ its', 'life 72zn']\n",
      "# of keys 370435\n",
      "['haven firefighters', 'in course', 'nikolic found', 'what respiratory', 'royal educated', 'constructed bitumont', 'nbc secure', 'what awarded', 'employ its', 'life 72zn']\n"
     ]
    }
   ],
   "source": [
    "keys = list(questionsVocabulary.keys())\n",
    "print(\"# of keys\", len(keys))\n",
    "print(keys[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article titles binarized\n",
      "Article titles binarized\n"
     ]
    }
   ],
   "source": [
    "# Training set.\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "combinedArticleTitles = pd.concat((trainingData[\"title\"],devData[\"title\"]))\n",
    "# I want the article titles to be trained based on words, so I need to split the string so MultiLabelBinarizer sees the words.\n",
    "binarizer = MultiLabelBinarizer().fit(combinedArticleTitles.str.split(\" \"))\n",
    "print(\"Article titles binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  761\n",
      "['%' '(' ')' ',' '.' '1973' '2004' '2007' '2008' '2015' '26' '27' '27s'\n",
      " '3' '360' '50' '51st' '8' '80' '9308']\n",
      "Number of classes:  761\n",
      "['%' '(' ')' ',' '.' '1973' '2004' '2007' '2008' '2015' '26' '27' '27s'\n",
      " '3' '360' '50' '51st' '8' '80' '9308']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes: \",len(binarizer.classes_))\n",
    "print(binarizer.classes_[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished binarizing article titles.\n",
      "Finished binarizing article titles.\n"
     ]
    }
   ],
   "source": [
    "trainingArticleTitlesBinarized = binarizer.transform(trainingData[\"title\"].str.split(\" \"))\n",
    "devArticleTitlesBinarized = binarizer.transform(devData[\"title\"].str.split(\" \"))\n",
    "print(\"Finished binarizing article titles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize article content (including answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered content; fitting binarizer\n",
      "Gathered content; fitting binarizer\n",
      "Binarizer fit. Transforming data.\n",
      "Binarizer fit. Transforming data.\n",
      "Data transformed.\n",
      "Data transformed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "contextTotal = pd.concat((trainingData[\"context\"],devData[\"context\"]))\n",
    "print(\"Gathered content; fitting binarizer\")\n",
    "contextBinarizer = MultiLabelBinarizer().fit(contextTotal)\n",
    "print(\"Binarizer fit. Transforming data.\")\n",
    "trainingContextBinarized = contextBinarizer.transform(trainingData[\"context\"])\n",
    "devContextBinarized = contextBinarizer.transform(devData[\"context\"])\n",
    "print(\"Data transformed.\")\n",
    "#print(\"Content vectorizer fit.\")\n",
    "#print(\"Vocabulary: \" + str(articleContentVocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes 1420\n",
      "Number of classes 1420\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes\",len(contextBinarizer.classes_))\n",
    "#print(pd.Series(contextBinarizer.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarize the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering answers and formatting them.\n",
      "Processed answers. Binarizing answers\n",
      "Gathering answers and formatting them.\n",
      "Processed answers. Binarizing answers\n",
      "Finished binarizing answers.\n",
      "Finished binarizing answers.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\"Gathering answers and formatting them.\")\n",
    "print(\"Processed answers. Binarizing answers\")\n",
    "# Convert answer_start and answer_end columns to equal-width arrays containing ones and zeros.\n",
    "# Get the highest possible number across the two columns.\n",
    "highestNumber = pd.concat((trainingData[\"answer_end\"],devData[\"answer_end\"])).max()\n",
    "binaryNumberWidth = int(math.log(highestNumber,2)) + 1\n",
    "binarize = lambda x: [int(y) for y in np.binary_repr(int(x),width=binaryNumberWidth)]\n",
    "trainingAnswerStartsBinarized = np.matrix(trainingData[\"answer_start\"].map(binarize).tolist())\n",
    "trainingAnswerEndsBinarized = np.matrix(trainingData[\"answer_end\"].map(binarize).tolist())\n",
    "devAnswerStartsBinarized = np.matrix(devData[\"answer_start\"].map(binarize).tolist())\n",
    "devAnswerEndsBinarized = np.matrix(devData[\"answer_end\"].map(binarize).tolist())\n",
    "#trainingAnswerEndsBinarized = pd.get_dummies(trainingAnswers[\"answer_end\"])\n",
    "#devAnswerStartsBinarized = pd.get_dummies(devAnswers[\"answer_start\"])\n",
    "#devAnswerEndsBinarized = pd.get_dummies(devAnswers[\"answer_end\"])\n",
    "print(\"Finished binarizing answers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 1 1 1 1 0 0]]\n",
      "(87355, 12)\n",
      "[[0 0 0 0 1 1 0 1 0 1 0 1]]\n",
      "(87355, 12)\n",
      "[[0 0 0 0 1 0 1 1 1 1 0 0]]\n",
      "(87355, 12)\n",
      "[[0 0 0 0 1 1 0 1 0 1 0 1]]\n",
      "(87355, 12)\n"
     ]
    }
   ],
   "source": [
    "#Test to make sure the code produces 1 answer for each row of records in the original DataFrames.\n",
    "print(trainingAnswerStartsBinarized[1])\n",
    "print(trainingAnswerStartsBinarized.shape)\n",
    "print(trainingAnswerEndsBinarized[1])\n",
    "print(trainingAnswerEndsBinarized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into testing and cross-validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation split happened on dataset 1!\n",
      "Cross-validation split happened on dataset 1!\n"
     ]
    }
   ],
   "source": [
    "# 20% of data will be set aside for cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_1_train,X_1_cross_validation,Y_1_train,Y_1_cross_validation = train_test_split(training_questions,trainingArticleTitlesBinarized)\n",
    "print(\"Cross-validation split happened on dataset 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 2 horizontally stacked.\n",
      "X 2 horizontally stacked.\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix,hstack\n",
    "X_2_train_cross_validation = hstack((training_questions,csr_matrix(trainingContextBinarized)))\n",
    "print(\"X 2 horizontally stacked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished train test split on dataset 2.\n",
      "[[0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1]]\n",
      "Finished train test split on dataset 2.\n",
      "[[0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix,hstack\n",
    "trainingAnswerStartsBinarizedCSRMatrix = csr_matrix(trainingAnswerStartsBinarized)\n",
    "trainingAnswerEndsBinarizedCSRMatrix = csr_matrix(trainingAnswerEndsBinarized)\n",
    "Y_2_train_cross_validation = hstack((trainingAnswerStartsBinarizedCSRMatrix,trainingAnswerEndsBinarizedCSRMatrix))\n",
    "X_2_train,X_2_cross_validation,Y_2_train,Y_2_cross_validation = train_test_split(X_2_train_cross_validation,Y_2_train_cross_validation)\n",
    "print(\"Finished train test split on dataset 2.\")\n",
    "print(Y_2_train_cross_validation.todense()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_2_dev = np.hstack((dev_questions.todense(),devContextBinarized))\n",
    "print(\"X 2 dev horizontally stacked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed datasets to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save(\"X_1_train.npz\",X_1_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
