{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Just to verify that everything's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9223372036854775807\n",
      "Set up!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Activation,InputLayer,Input,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from IPython.display import display\n",
    "# Verify we are using GPU.\n",
    "from tensorflow.python.client import device_lib\n",
    "import sys\n",
    "print(sys.maxsize)\n",
    "assert sys.version_info[0] >= 3\n",
    "print(\"Set up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing step, we will create two dictionaries. One will be used to map questions to articles, while the other will be used to map questions and the contents of the articles to the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading JSON datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "def readFile(filename):\n",
    "    with open(filename) as file:\n",
    "        data = json.load(file)[\"data\"]\n",
    "        paragraphs = pd.io.json.json_normalize(data,record_path=\"paragraphs\")\n",
    "        # Map paragraph IDs and qas IDs.\n",
    "        paragraphIDs_questions = []\n",
    "        lastParagraphId = 0\n",
    "        for index,paragraph in paragraphs.iterrows():\n",
    "            for qasElement in paragraph[\"qas\"]:\n",
    "                paragraphIDs_questions.append({\"paragraphID\":lastParagraphId,\"question\":qasElement[\"question\"]})\n",
    "            lastParagraphId = lastParagraphId + 1\n",
    "        paragraphIDs_questions = pd.DataFrame(paragraphIDs_questions)\n",
    "        print(\"Finished mapping paragraph IDs to qas questions. Now generating qas dataframe and doing other things.\")\n",
    "        qas = pd.io.json.json_normalize(data,record_path=[\"paragraphs\",\"qas\"],meta=[\"title\"])\n",
    "        paragraphs[\"id\"] = paragraphs.index\n",
    "        #print(qas[\"question\"])\n",
    "        #Gather a list of where all answers should be so we can shove them into a DataFrame.\n",
    "        # Haven't found a more efficient way to do this yet.\n",
    "        answer_ids = set()\n",
    "        answerId = 0\n",
    "        for index,row in qas.iterrows():\n",
    "            answer_ids.add(answerId)\n",
    "            answerId = answerId + len(row[\"answers\"])\n",
    "        print(\"Finished with answer ids.\")\n",
    "        # Map qas pair IDs to answer IDs.\n",
    "        answer_ids = pd.DataFrame(list(answer_ids))\n",
    "        print(\"Finished converting answer_ids to DataFrame.\")\n",
    "        question_answerId = pd.DataFrame(qas[\"question\"]).join(answer_ids,how=\"outer\")\n",
    "        question_answerId.columns = [\"question\",\"answer_id\"]\n",
    "        #print(\"Id-answerID columns: \",id_answerId.columns)\n",
    "        print(\"finished creating intermediary table.\")\n",
    "        # Load answers into a data frame.\n",
    "        answers = pd.io.json.json_normalize(data,record_path=[\"paragraphs\",\"qas\",\"answers\"])\n",
    "        answers.rename(columns={\"text\":\"answer_text\"},inplace=True)\n",
    "        # Give each answer an ID.\n",
    "        answers[\"id\"] = answers.index\n",
    "        print(\"Finished creating answers dataframe.\")\n",
    "        qas = qas.drop(labels=[\"answers\"],axis=1) # Not needed any longer; we have the answers!\n",
    "        #print(\"Dropped column 'answers' from qas.\")\n",
    "        # Map qas dataframe to answer table via id_answerId\n",
    "        qas_answerId = pd.merge(qas,question_answerId,how=\"inner\",on=\"question\")\n",
    "        print(\"Finished joining qas to answer id\")\n",
    "        # Merge qas_answerId with answers.\n",
    "        qas = pd.merge(qas_answerId,answers,how=\"inner\",left_on=\"answer_id\",right_on=\"id\")\n",
    "        #print(\"Returned data frame: \",returnDataFrame)\n",
    "        # Finally, include context.\n",
    "        context = paragraphs[[\"context\",\"id\"]]\n",
    "        qasWithParagraphID = pd.merge(qas,paragraphIDs_questions,how=\"inner\",on=\"question\")\n",
    "        qas = pd.merge(qasWithParagraphID,context,how=\"inner\",left_on=\"paragraphID\",right_on=\"id\")\n",
    "        qas = qas.drop_duplicates(\"question\")\n",
    "        assert qas.duplicated(\"question\").any() == False\n",
    "        # Prune the dataframe.\n",
    "        # NOTE: Answer_start column may end up being useful.\n",
    "        qas = qas.drop(labels=[\"id_x\",\"id_y\",\"answer_id\",\"paragraphID\",\"id\"],axis=1)\n",
    "        # Remove punctuation from all text columns.\n",
    "        qas[\"title\"] = qas['title'].str.replace('_',' ')\n",
    "        qas[\"title\"] = qas['title'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "        qas[\"question\"] = qas['question'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "        qas[\"context\"] = qas['context'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "        qas[\"answer_text\"] = qas['answer_text'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "        print(\"Done!\")\n",
    "        return qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = readFile(\"train-v1.1.json\")\n",
    "display(trainingData)\n",
    "from sys import getsizeof\n",
    "print(\"Finished loading training data.\")\n",
    "print(\"Size of training data:\" + str(getsizeof(trainingData) / 1024**2) + \" MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dev_questions) == len(dev_answers)\n",
    "print(\"Amount of dev data: \" + str(len(dev_questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing using scikit-learn and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section turns inputs into n-grams. Note that order is not necessarily preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom skip-gram vectorizer\n",
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "    \n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        \"\"\"Turn tokens into a sequence of 1-skip-2-grams after stop words filtering\"\"\"\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(trainData,devData):\n",
    "    vectorizer = SkipGramVectorizer()\n",
    "    combinedData = pd.concat((trainData,devData))\n",
    "    vectorizer.fit(combinedData)\n",
    "    print(\"Vectorizer fit.\")\n",
    "    trainData_vectorized = vectorizer.transform(trainData)\n",
    "    print(\"First text vectorized\")\n",
    "    devData_vectorized = vectorizer.transform(devData)\n",
    "    print(\"Second text vectorized\")\n",
    "    return trainData_vectorized,devData_vectorized,vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Vectorize all questions.\n",
    "training_questions,dev_questions,vocabulary1 = vectorize(trainingData[\"question\"],devData[\"question\"])\n",
    "print(\"Number of features:\",training_questions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set.\n",
    "training_articleTitles,dev_articleTitles,articleTitles_vocabulary = vectorize(trainingData[\"title\"],devData[\"title\"])\n",
    "print(\"Article titles vectorized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize article content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer,LabelBinarizer\n",
    "contentTotal = pd.concat((trainingData[\"context\"],devData[\"context\"]))\n",
    "print(\"Gathered content\")\n",
    "series_split = lambda series: series.apply(lambda x: x.split(\" \"))\n",
    "print(series_split(pd.Series(\"Who are you\"))) # Test code.\n",
    "contentTotal = series_split(contentTotal)\n",
    "contentBinarizer = MultiLabelBinarizer()\n",
    "contentBinarizer.fit(contentTotal)\n",
    "test = contentBinarizer.inverse_transform(contentBinarizer.transform(series_split(pd.Series(\"Saint Bernadette\"))))\n",
    "print(\"Test\",test)\n",
    "print(\"Content binarizer fit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_context = contentBinarizer.transform(series_split(trainingData[\"context\"]))\n",
    "print(\"Training context binarized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_context = contentBinarizer.transform(series_split(devData[\"context\"]))\n",
    "print(\"Dev context binarized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarize the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersBinarizer = MultiLabelBinarizer()\n",
    "combinedAnswers = pd.concat((trainingData[\"answer_text\"],devData[\"answer_text\"]))\n",
    "answersBinarizer.fit(series_split(combinedAnswers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_answers = answersBinarizer.transform(series_split(trainingData[\"answer_text\"]))\n",
    "print(\"Training answers binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_answers = answersBinarizer.transform(series_split(devData[\"answer_text\"]))\n",
    "print(\"Dev answers binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features for questions vectorizer:\",len(vocabulary1))\n",
    "print(\"Number of features in article titles:\",len(articleTitles_vocabulary))\n",
    "print(\"Number of features in context:\",len(contentBinarizer.classes_))\n",
    "print(\"Number of features in answers: \",len(answersBinarizer.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've vectorized the questions and binarized the article context, we have one problem. First, the questions are represented by ordinal numbers. Our neural network will work best if the questions are represented as binary numbers; i.e, they contain only ones and zeroes. Second, each question in the dataset has 371,624 features. The dataset itself contains approximately 90,000 entries. This is a common problem in data science; having more features than data can lead the algorithm to miss significant relationships between the features and the end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "training_questions = normalize(training_questions)\n",
    "dev_questions = normalize(dev_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to numpy arrays, get input shape\n",
    "inputShape_first = training_questions.toarray().shape[1:] # Input shape of the first neural network.\n",
    "article_titles_shape = training_articleTitles.toarray().shape\n",
    "print(\"First neural network X shape: \" + str(training_questions.shape))\n",
    "print(\"First neural network input shape: \" + str(inputShape_first))\n",
    "print(\"Article titles array shape: \" + str(article_titles_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into testing and cross-validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% of data will be set aside for cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_1_train,X_1_cross_validation,Y_1_train,Y_1_cross_validation = train_test_split(training_questions,training_articleTitles)\n",
    "print(type(X_1_train))\n",
    "print(X_1_train.shape)\n",
    "print(Y_1_train.shape)\n",
    "print(X_1_cross_validation.shape)\n",
    "print(Y_1_cross_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "X_2 = hstack((training_questions,training_articleTitles))\n",
    "print(\"X 2 horizontally stacked.\")\n",
    "X_2_train,X_2_cross_validation,Y_2_train,Y_2_cross_validation = train_test_split(X_2,training_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2_dev = hstack((dev_questions,dev_articleTitles))\n",
    "print(\"X 2 dev horizontally stacked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed datasets to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz(\"X_1_train\",X_1_train)\n",
    "save_npz(\"X_1_cross_validation\",X_1_cross_validation)\n",
    "save_npz(\"dev_questions\",dev_questions)\n",
    "save_npz(\"X_2_train\",X_2_train)\n",
    "save_npz(\"X_2_cross_validation\",X_2_cross_validation)\n",
    "save_npz(\"X_2_dev\",X_2_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz(\"Y_1_train\",Y_1_train)\n",
    "save_npz(\"Y_1_cross_validation\",Y_1_cross_validation)\n",
    "save_npz(\"dev_articleTitles\",dev_articleTitles)\n",
    "save_npz(\"Y_2_train\",csr_matrix(Y_2_train))\n",
    "save_npz(\"Y_2_cross_validation\",csr_matrix(Y_2_cross_validation))\n",
    "save_npz(\"dev_answers\",csr_matrix(dev_answers))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-preprocessing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabularies from preprocessors above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
